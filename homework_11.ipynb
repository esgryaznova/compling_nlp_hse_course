{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esgryaznova/compling_nlp_hse_course/blob/master/homework_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dba7c0d",
      "metadata": {
        "id": "1dba7c0d"
      },
      "source": [
        "# Домашнее задание № 11. Машинный перевод"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yj7aripVIsbG",
      "metadata": {
        "id": "Yj7aripVIsbG"
      },
      "source": [
        "## Задание 1 (6 баллов + 2 доп балла).\n",
        "Нужно обучить трансформер на этом же или на другом корпусе (можно взять другую языковую пару с того же сайте) и оценивать его на всей тестовой выборке (а не на 10 примерах как сделал я). \n",
        "\n",
        "Чтобы получить 2 доп балла вам нужно будет придумать как оптимизировать функцию translate. Подсказка: модель может предсказывать батчами.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05d202c4",
      "metadata": {
        "id": "05d202c4"
      },
      "outputs": [],
      "source": [
        "!pip install tokenizers matplotlib sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "vb6awS7OU8e2"
      },
      "id": "vb6awS7OU8e2",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "de_sents = open('opus.de-en-train.de.txt').read().splitlines()\n",
        "en_sents = open('opus.de-en-train.en.txt').read().splitlines()"
      ],
      "metadata": {
        "id": "mYgjt338VPGV"
      },
      "id": "mYgjt338VPGV",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "de_sents[-1], en_sents[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebQqQsjXZuQn",
        "outputId": "e6036eaf-4645-4ee5-d3cd-73d5dd3b6aed"
      },
      "id": "ebQqQsjXZuQn",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Es geht nicht darum, hier zu kriminalisieren, zu emotionalisieren, sondern wir müssen uns mit dem Thema auseinandersetzen.',\n",
              " 'I am not talking about criminalising it or reacting to it emotionally, but we must tackle this subject.')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_de = Tokenizer(BPE())\n",
        "tokenizer_de.pre_tokenizer = Whitespace()\n",
        "trainer_de = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "tokenizer_de.train(files=[\"opus.de-en-train.de.txt\"], trainer=trainer_de)\n",
        "\n",
        "tokenizer_en = Tokenizer(BPE())\n",
        "tokenizer_en.pre_tokenizer = Whitespace()\n",
        "trainer_en = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "tokenizer_en.train(files=[\"opus.de-en-train.en.txt\"], trainer=trainer_en)"
      ],
      "metadata": {
        "id": "fG0Uuc44aGFT"
      },
      "id": "fG0Uuc44aGFT",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# раскоментируйте эту ячейку при обучении токенизатора\n",
        "# а потом снова закоментируйте чтобы при перезапуске не перезаписать токенизаторы\n",
        "tokenizer_de.save('tokenizer_de')\n",
        "tokenizer_en.save('tokenizer_en')"
      ],
      "metadata": {
        "id": "pFOf8YODaGXt"
      },
      "id": "pFOf8YODaGXt",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_de = Tokenizer.from_file(\"tokenizer_de\")\n",
        "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")"
      ],
      "metadata": {
        "id": "eFwv7t-0bOm4"
      },
      "id": "eFwv7t-0bOm4",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text, tokenizer, max_len):\n",
        "    return [tokenizer.token_to_id('[CLS]')] + tokenizer.encode(text).ids[:max_len] + [tokenizer.token_to_id('[SEP]')]"
      ],
      "metadata": {
        "id": "EVPfbNP1bWjZ"
      },
      "id": "EVPfbNP1bWjZ",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# важно следить чтобы индекс паддинга совпадал в токенизаторе с value в pad_sequences\n",
        "PAD_IDX = tokenizer_en.token_to_id('[PAD]')\n",
        "PAD_IDX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEY8TvLibaDc",
        "outputId": "6943cea4-b318-42ad-a30a-241e9af26bd3"
      },
      "id": "nEY8TvLibaDc",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ограничимся длинной в 30 и 35 (разные чтобы показать что в seq2seq не нужна одинаковая длина)\n",
        "max_len_de, max_len_en = 30, 35"
      ],
      "metadata": {
        "id": "WBVJ5Co0cEa0"
      },
      "id": "WBVJ5Co0cEa0",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_de = [encode(t, tokenizer_de, max_len_de) for t in de_sents]\n",
        "X_en = [encode(t, tokenizer_en, max_len_en) for t in en_sents]"
      ],
      "metadata": {
        "id": "kHFZLPBScHM9"
      },
      "id": "kHFZLPBScHM9",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, texts_de, texts_en):\n",
        "        self.texts_de = [torch.LongTensor(sent) for sent in texts_de]\n",
        "        self.texts_de = torch.nn.utils.rnn.pad_sequence(self.texts_de, padding_value=PAD_IDX)\n",
        "        \n",
        "        self.texts_en = [torch.LongTensor(sent) for sent in texts_en]\n",
        "        self.texts_en = torch.nn.utils.rnn.pad_sequence(self.texts_en, padding_value=PAD_IDX)\n",
        "\n",
        "        self.length = len(texts_de)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        ids_de = self.texts_de[:, index]\n",
        "        ids_en = self.texts_en[:, index]\n",
        "\n",
        "        return ids_de, ids_en"
      ],
      "metadata": {
        "id": "_65e5dbVcjdI"
      },
      "id": "_65e5dbVcjdI",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_de_train, X_de_valid, X_en_train, X_en_valid = train_test_split(X_de, X_en, test_size=0.05)"
      ],
      "metadata": {
        "id": "4zNLShJldDII"
      },
      "id": "4zNLShJldDII",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = Dataset(X_de_train, X_en_train)\n",
        "training_generator = torch.utils.data.DataLoader(training_set, batch_size=200, shuffle=True, )"
      ],
      "metadata": {
        "id": "gUAknrAFsEsI"
      },
      "id": "gUAknrAFsEsI",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_set = Dataset(X_de_valid, X_en_valid)\n",
        "valid_generator = torch.utils.data.DataLoader(valid_set, batch_size=200, shuffle=True)"
      ],
      "metadata": {
        "id": "cyNoM2tCsFAD"
      },
      "id": "cyNoM2tCsFAD",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 150):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size, \n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "#         print('pos inp')\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "#         print('pos dec')\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "#         print('pos out')\n",
        "        x = self.generator(outs)\n",
        "#         print('gen')\n",
        "        return x\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)\n",
        "# During training, we need a subsequent word mask that will prevent model to look into the future words when making predictions. We will also need masks to hide source and target padding tokens. Below, let’s define a function that will take care of both.\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    \n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "metadata": {
        "id": "fDbdkXTSsFRk"
      },
      "id": "fDbdkXTSsFRk",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "def train(model, iterator, optimizer, criterion, print_every=500):\n",
        "    \n",
        "    epoch_loss = []\n",
        "    ac = []\n",
        "    \n",
        "    model.train()  \n",
        "\n",
        "    for i, (texts_de, texts_en) in enumerate(iterator):\n",
        "        texts_de = texts_de.T.to(DEVICE) # чтобы батч был в конце\n",
        "        texts_en = texts_en.T.to(DEVICE) # чтобы батч был в конце\n",
        "        \n",
        "        # помимо текста в модель еще нужно передать целевую последовательность\n",
        "        # но не полную а без 1 последнего элемента\n",
        "        # а на выходе ожидаем, что модель сгенерирует этот недостающий элемент\n",
        "        texts_en_input = texts_en[:-1, :]\n",
        "        \n",
        "        \n",
        "        # в трансформерах нет циклов как в лстм \n",
        "        # каждый элемент связан с каждым через аттеншен\n",
        "        # чтобы имитировать последовательную обработку\n",
        "        # и чтобы не считать аттеншн с паддингом \n",
        "        # в трансформерах нужно считать много масок\n",
        "        # подробнее про это по ссылкам выше\n",
        "        (texts_de_mask, texts_en_mask, \n",
        "        texts_de_padding_mask, texts_en_padding_mask) = create_mask(texts_de, texts_en_input)\n",
        "        logits = model(texts_de, texts_en_input, texts_de_mask, texts_en_mask,\n",
        "                       texts_de_padding_mask, texts_en_padding_mask, texts_de_padding_mask)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # сравниваем выход из модели с целевой последовательностью уже с этим последним элементом\n",
        "        texts_en_out = texts_en[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), texts_en_out.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss.append(loss.item())\n",
        "        \n",
        "        if not (i+1) % print_every:\n",
        "            print(f'Loss: {np.mean(epoch_loss)};')\n",
        "        \n",
        "    return np.mean(epoch_loss)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = []\n",
        "    epoch_f1 = []\n",
        "    \n",
        "    model.eval()  \n",
        "    with torch.no_grad():\n",
        "        for i, (texts_en, texts_ru) in enumerate(iterator):\n",
        "            texts_en = texts_en.T.to(DEVICE)\n",
        "            texts_ru = texts_ru.T.to(DEVICE)\n",
        "\n",
        "            texts_ru_input = texts_ru[:-1, :]\n",
        "\n",
        "            (texts_en_mask, texts_ru_mask, \n",
        "            texts_en_padding_mask, texts_ru_padding_mask) = create_mask(texts_en, texts_ru_input)\n",
        "\n",
        "            logits = model(texts_en, texts_ru_input, texts_en_mask, texts_ru_mask,\n",
        "                           texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
        "\n",
        "            \n",
        "            texts_ru_out = texts_ru[1:, :]\n",
        "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), texts_ru_out.reshape(-1))\n",
        "            epoch_loss.append(loss.item())\n",
        "            \n",
        "    return np.mean(epoch_loss)"
      ],
      "metadata": {
        "id": "z-hnREF0sFcD"
      },
      "id": "z-hnREF0sFcD",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "DE_VOCAB_SIZE = tokenizer_de.get_vocab_size()\n",
        "EN_VOCAB_SIZE = tokenizer_en.get_vocab_size()\n",
        "\n",
        "EMB_SIZE = 256\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "NUM_ENCODER_LAYERS = 2\n",
        "NUM_DECODER_LAYERS = 2\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, DE_VOCAB_SIZE, EN_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "metadata": {
        "id": "7qmlmmv4tSC_"
      },
      "id": "7qmlmmv4tSC_",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "jn00VFXvtSNF"
      },
      "id": "jn00VFXvtSNF",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train(transformer, training_generator, optimizer, loss_fn)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer, valid_generator, loss_fn)\n",
        "    \n",
        "    if not losses:\n",
        "        print(f'First epoch - {val_loss}, saving model..')\n",
        "        torch.save(transformer, 'model')\n",
        "    \n",
        "    elif val_loss < min(losses):\n",
        "        print(f'Improved from {min(losses)} to {val_loss}, saving model..')\n",
        "        torch.save(transformer, 'model')\n",
        "    \n",
        "    losses.append(val_loss)\n",
        "        \n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \\\n",
        "           \"f\"Epoch time={(end_time-start_time):.3f}s\"))\n",
        "\n",
        "#тут обучились на 4 эпохах где-то"
      ],
      "metadata": {
        "id": "XFVXAojytSUX"
      },
      "id": "XFVXAojytSUX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text):\n",
        "\n",
        "\n",
        "    input_ids = [tokenizer_de.token_to_id('[CLS]')] + tokenizer_de.encode(text).ids[:max_len_de] + [tokenizer_de.token_to_id('[SEP]')]\n",
        "    output_ids = [tokenizer_en.token_to_id('[CLS]')]\n",
        "\n",
        "    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(input_ids)]).to(DEVICE)\n",
        "    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)]).to(DEVICE)\n",
        "\n",
        "    (texts_de_mask, texts_en_mask, \n",
        "    texts_de_padding_mask, texts_en_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
        "    logits = transformer(input_ids_pad, output_ids_pad, texts_de_mask, texts_en_mask,\n",
        "                   texts_de_padding_mask, texts_en_padding_mask, texts_de_padding_mask)\n",
        "    pred = logits.argmax(2).item()\n",
        "\n",
        "    while pred not in [tokenizer_en.token_to_id('[SEP]'), tokenizer_en.token_to_id('[PAD]')]:\n",
        "        output_ids.append(pred)\n",
        "        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)]).to(DEVICE)\n",
        "\n",
        "        (texts_de_mask, texts_en_mask, \n",
        "        texts_de_padding_mask, texts_en_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
        "        logits = transformer(input_ids_pad, output_ids_pad, texts_de_mask, texts_en_mask,\n",
        "                       texts_de_padding_mask, texts_en_padding_mask, texts_de_padding_mask)\n",
        "        pred = logits.argmax(2)[-1].item()\n",
        "\n",
        "    return (' '.join([tokenizer_en.id_to_token(i).replace('##', '') for i in output_ids[1:]]))\n",
        "\n"
      ],
      "metadata": {
        "id": "Id5cf0devaNA"
      },
      "id": "Id5cf0devaNA",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"alles klar, Herr Komissar?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bUs_1c2fwAfR",
        "outputId": "4d042e21-2178-4db0-a096-c6105c5223f6"
      },
      "id": "bUs_1c2fwAfR",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'All right , Mr . Com iss ar ?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in de_sents[:100]:\n",
        "    print('Deutsch: ', sent)\n",
        "    print('English: ', translate(sent))\n",
        "    print( )\n",
        "\n",
        "#всё круто, я в восторге !! правда некоторый перевод неправильный, похож на гугл перевод из 2010"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVcIn82lwD2Y",
        "outputId": "cd01d77a-6970-40c8-d712-47628fe2fd37"
      },
      "id": "HVcIn82lwD2Y",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deutsch:  Deine Habgier wird noch dein Tod sein.\n",
            "English:  Your Majesty ' s gonna be your death .\n",
            "\n",
            "Deutsch:  - Vega.\n",
            "English:  - Ve ga .\n",
            "\n",
            "Deutsch:  Sagen Sie einfach stopp.\n",
            "English:  Just tell you ,\n",
            "\n",
            "Deutsch:  - Warte.\n",
            "English:  - Wait .\n",
            "\n",
            "Deutsch:  Ich will nicht hier sein.\n",
            "English:  I don ' t want to be here .\n",
            "\n",
            "Deutsch:  Also, 90 Prozent meiner fotografischen Arbeit ist genau genommen gar nicht fotografisch.\n",
            "English:  So , 90 % of my photo of my photo work is not photograph ed .\n",
            "\n",
            "Deutsch:  Wie bezaubernd.\n",
            "English:  Like a nice .\n",
            "\n",
            "Deutsch:  Vielen Dank, Colonel.\n",
            "English:  Thank you , Colonel .\n",
            "\n",
            "Deutsch:  Martin!\n",
            "English:  Martin !\n",
            "\n",
            "Deutsch:  Muss ich?\n",
            "English:  I ' m not a good thing .\n",
            "\n",
            "Deutsch:  Nehmen Sie Platz.\n",
            "English:  Take place .\n",
            "\n",
            "Deutsch:  Leo!\n",
            "English:  Leo !\n",
            "\n",
            "Deutsch:  Nein, ehrlich gesagt finde ich, dass du herausfinden solltest, was mit deinem Dad los ist.\n",
            "English:  No , I think I ' m sure you ' re going to figure out what ' s your dad ' s going on .\n",
            "\n",
            "Deutsch:  Wie halten Sie sich?\n",
            "English:  How do you think ?\n",
            "\n",
            "Deutsch:  - Fangen wir an.\n",
            "English:  - Let ' s start .\n",
            "\n",
            "Deutsch:  Ich darf wirklich aufstehen, Herr Hofrat?\n",
            "English:  I can really get up , Mr H fr at ?\n",
            "\n",
            "Deutsch:  - Was machst du damit?\n",
            "English:  - What do you do ?\n",
            "\n",
            "Deutsch:  - Können wir reden?\n",
            "English:  - Can we talk ?\n",
            "\n",
            "Deutsch:  Runter!\n",
            "English:  Get down !\n",
            "\n",
            "Deutsch:  Und wenn es nur eine Störung war, weil Pamela Raker eine Republikanerin ist?\n",
            "English:  And if it was a chance to be a dis appoint , i . e . g . it ' s a man who ' s daughter ?\n",
            "\n",
            "Deutsch:  - Gern.\n",
            "English:  - You ' re welcome .\n",
            "\n",
            "Deutsch:  in Erwägung nachstehenden Grundes:\n",
            "English:  Whereas :\n",
            "\n",
            "Deutsch:  Wir haben ferngesehen.\n",
            "English:  We ' ve seen the little .\n",
            "\n",
            "Deutsch:  - Danke. - Okay.\n",
            "English:  - Thank you .\n",
            "\n",
            "Deutsch:  22:35\n",
            "English:  21 : 35\n",
            "\n",
            "Deutsch:  12:12\n",
            "English:  12 : 15\n",
            "\n",
            "Deutsch:  Exzellent.\n",
            "English:  Excellent .\n",
            "\n",
            "Deutsch:  Na schön.\n",
            "English:  All right .\n",
            "\n",
            "Deutsch:  Tag 1\n",
            "English:  Day 1\n",
            "\n",
            "Deutsch:  Artikel 32\n",
            "English:  Article 32\n",
            "\n",
            "Deutsch:  Lebt wohl.\n",
            "English:  I think it ' s a lot .\n",
            "\n",
            "Deutsch:  Werte\n",
            "English:  Values\n",
            "\n",
            "Deutsch:  Oder was meinst du?\n",
            "English:  Or what do you mean ?\n",
            "\n",
            "Deutsch:  2011-02-05, 06:02 PM #1\n",
            "English:  2011 - 02 - 05 , 06 : 02 PM # 1\n",
            "\n",
            "Deutsch:  Sie wollen was.\n",
            "English:  You want to be what ?\n",
            "\n",
            "Deutsch:  Eins, zwei, eins, zwei.\n",
            "English:  One , two , two , two .\n",
            "\n",
            "Deutsch:  KOGENATE Bayer wurde in klinischen Studien zur Behandlung von Blutungen bei 37 nicht vorbehandelten (PUPs) und 23 minimal vorbehandelten Hämophilie-Patienten (MTPs, Patienten mit 4 oder weniger Expositionstagen) eingesetzt.\n",
            "English:  KO GEN ATE Bayer has been used in clinical studies in clinical trials in the treatment of blood - related blood ( P UP s ) and 23 minimal in patients .\n",
            "\n",
            "Deutsch:  48:02\n",
            "English:  01 : 38\n",
            "\n",
            "Deutsch:  Tagungswoche\n",
            "English:  The Week\n",
            "\n",
            "Deutsch:  Deine Zeit wird kommen.\n",
            "English:  Your time will come .\n",
            "\n",
            "Deutsch:  Eine große Geste. - Worum geht es dir?\n",
            "English:  A big deal .\n",
            "\n",
            "Deutsch:  Wo wollen Sie denn hin, Stone?\n",
            "English:  Where are you going , Stone ?\n",
            "\n",
            "Deutsch:  Keine Bewegung.\n",
            "English:  No movement .\n",
            "\n",
            "Deutsch:  Ja, natürlich.\n",
            "English:  Yeah , of course .\n",
            "\n",
            "Deutsch:  Die Autoschlüssel.\n",
            "English:  The car key .\n",
            "\n",
            "Deutsch:  Wir könnten überall in der Galaxie herauskommen.\n",
            "English:  We could get out of the galaxy .\n",
            "\n",
            "Deutsch:  - Äh...\n",
            "English:  - Uh ...\n",
            "\n",
            "Deutsch:  - MANN: Tut mir leid.\n",
            "English:  - I ' m sorry .\n",
            "\n",
            "Deutsch:  Gib es mir!\n",
            "English:  Give me it !\n",
            "\n",
            "Deutsch:  ÇáÓÇÚÉ ÇáÂä: 08:11 PM\n",
            "English:  ÇáÓÇÚÉ ÇáÂä : 06 : 11 PM\n",
            "\n",
            "Deutsch:  Pumbaa, du bist 'n Schwein.\n",
            "English:  P um ba a , you ' re ' n pig .\n",
            "\n",
            "Deutsch:  Übrigens:\n",
            "English:  By way :\n",
            "\n",
            "Deutsch:  Heiß!\n",
            "English:  Hot !\n",
            "\n",
            "Deutsch:  Wie war das?\n",
            "English:  How was that ?\n",
            "\n",
            "Deutsch:  Bring mal etwas Wasser.\n",
            "English:  Take a little water .\n",
            "\n",
            "Deutsch:  Aber jetzt nicht mehr.\n",
            "English:  But now not more .\n",
            "\n",
            "Deutsch:  - Das stimmt.\n",
            "English:  - That ' s right .\n",
            "\n",
            "Deutsch:  Pink für dich, Blau für Luke.\n",
            "English:  Pink for you , blue for Luke .\n",
            "\n",
            "Deutsch:  - Ganz ruhig bleiben.\n",
            "English:  - Calm down .\n",
            "\n",
            "Deutsch:  Allahu Akbar.\n",
            "English:  Allah u ac .\n",
            "\n",
            "Deutsch:  Ja!\n",
            "English:  Yes !\n",
            "\n",
            "Deutsch:  Weißt du, wer Remy Danton ist?\n",
            "English:  You know who Rem y D anton is ?\n",
            "\n",
            "Deutsch:  Ich möchte jetzt gehen.\n",
            "English:  I want to go .\n",
            "\n",
            "Deutsch:  - Sicher.\n",
            "English:  - Sure .\n",
            "\n",
            "Deutsch:  Ich darf mich für eure Geduld bedanken.\n",
            "English:  I should like to thank you for your patience .\n",
            "\n",
            "Deutsch:  Bob, Sie haben da was am Anzug.\n",
            "English:  Bob , you have something on the suit .\n",
            "\n",
            "Deutsch:  Sieht gut aus.\n",
            "English:  It looks good .\n",
            "\n",
            "Deutsch:  - Sid.\n",
            "English:  - Sid .\n",
            "\n",
            "Deutsch:  - Ein Bündnis mit wem?\n",
            "English:  - A Alliance with whom ?\n",
            "\n",
            "Deutsch:  - Hast du das gesehen? - Was?\n",
            "English:  - What ?\n",
            "\n",
            "Deutsch:  - Mnh-mnh.\n",
            "English:  - M n h - m h .\n",
            "\n",
            "Deutsch:  02:57\n",
            "English:  02 : 57\n",
            "\n",
            "Deutsch:  Mindestens 90 %; sekundäre Komponenten 4-6 % Geraniol und 1-2 % Nerol\n",
            "English:  At least 90 % ; se - 6 % of the 4 - 6 % Ger ani ol and 1 % - 2 % N ol\n",
            "\n",
            "Deutsch:  Microstomus kitt\n",
            "English:  Micro st om us k itt\n",
            "\n",
            "Deutsch:  Wir fahren.\n",
            "English:  We ' re going to go .\n",
            "\n",
            "Deutsch:  Drücken!\n",
            "English:  Press !\n",
            "\n",
            "Deutsch:  Geht schon.\n",
            "English:  - Come on .\n",
            "\n",
            "Deutsch:  Wissen Sie Bescheid über Manfredi und den Pakt?\n",
            "English:  You know about Man fre i and the Pact ?\n",
            "\n",
            "Deutsch:  Länge des GGB\n",
            "English:  Length of the G GB\n",
            "\n",
            "Deutsch:  - Ja. Genau so.\n",
            "English:  - Yeah .\n",
            "\n",
            "Deutsch:  Wow!\n",
            "English:  Wow !\n",
            "\n",
            "Deutsch:  Nein, das ist unmöglich.\n",
            "English:  No , it ' s impossible .\n",
            "\n",
            "Deutsch:  Sei willkommen, Bruder.\n",
            "English:  Welcome , bro .\n",
            "\n",
            "Deutsch:  16:13\n",
            "English:  16 : 13\n",
            "\n",
            "Deutsch:  Wer sind diese Männer?\n",
            "English:  Who are these men ?\n",
            "\n",
            "Deutsch:  Hört zu...\n",
            "English:  Listen ,\n",
            "\n",
            "Deutsch:  35:01\n",
            "English:  35 : 57\n",
            "\n",
            "Deutsch:  So. Ihr müsst den Rhythmus finden.\n",
            "English:  You have to find the ball .\n",
            "\n",
            "Deutsch:  - Hi.\n",
            "English:  - Hi .\n",
            "\n",
            "Deutsch:  Artikel 14 und möglichst\n",
            "English:  Article 14 and as\n",
            "\n",
            "Deutsch:  Durst!\n",
            "English:  L enn !\n",
            "\n",
            "Deutsch:  Weiter!\n",
            "English:  Go !\n",
            "\n",
            "Deutsch:  Erweitert\n",
            "English:  For the purposes of the European Parliament ,\n",
            "\n",
            "Deutsch:  Legen Sie die Endzeit fest.@option:check\n",
            "English:  Place the final time .\n",
            "\n",
            "Deutsch:  06:45\n",
            "English:  06 : 20\n",
            "\n",
            "Deutsch:  Es kommt näher.\n",
            "English:  It ' s close .\n",
            "\n",
            "Deutsch:  Oh, Sie armes Ding.\n",
            "English:  Oh , you poor thing .\n",
            "\n",
            "Deutsch:  Es\n",
            "English:  It ' s a man .\n",
            "\n",
            "Deutsch:  Sie würden herausfinden wollen: ,Bin ich allein?\"\n",
            "English:  You ' re going to find out : , B ' s alone ?\"\n",
            "\n",
            "Deutsch:  - Bis bald, Simon.\n",
            "English:  - See you , Simon .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5aa93d6",
      "metadata": {
        "id": "b5aa93d6"
      },
      "source": [
        "\n",
        "## Задание 2 (2 балла).\n",
        "Прочитайте главу про машинный перевод у Журафски и Маннига - https://web.stanford.edu/~jurafsky/slp3/10.pdf \n",
        "Ответьте своими словами в чем заключается техника back translation? Для чего она применяется и что позволяет получить? Опишите по шагам как его применить к паре en-ru на данных из семинара. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для обучения моделей машинного перевода необходимы параллельные корпуса, но их не так много, т.е. данных для обучения недостаточно. Поэтому можно взять монокорпус и просто добавить его к доступным параллельным корпусам. Эта техника заключается в том, чтобы создать битекст синтетически.\n",
        "Сначала модель обучается на маленьком битексте, чтобы перевести данные на таргетном языке на язык-источник. Этот синтетически созданный битекст добавляется к данным для обучения.\n",
        "\n",
        "То есть если хотим перевести текст с английского на русский, но у нас есть только маленький битекст, мы строим систему, чтобы перевести текст с русского на английский. Когда перевели текст на английский, то этот битекст теперь можно добавить к обучающим данным."
      ],
      "metadata": {
        "id": "nrKUGmvLdkd8"
      },
      "id": "nrKUGmvLdkd8"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "homework_11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}