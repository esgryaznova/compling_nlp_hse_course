{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esgryaznova/compling_nlp_hse_course/blob/master/homework_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dba7c0d"
      },
      "source": [
        "# Домашнее задание № 10. Генерация текста"
      ],
      "id": "1dba7c0d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76f21d5e"
      },
      "source": [
        "### Задание 1 (8 баллов).\n",
        "\n",
        "Попробуйте дообучать GPT на каком-то другом тексте (можете попробовать любые стихи или какие-то специфичные вещи вроде анекдотов, теорий заговоров, постов в помоечных телеграм каналах, текстов журналистов и СМИ с выразительным стилем). \n",
        "Попробуйте разные методы и параметры генерации (beam search, температура, top_k и тп). Сохраните в тетрадке несколько хороших сгенерированных текстов\n"
      ],
      "id": "76f21d5e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2444e3fe"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ],
      "id": "2444e3fe"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tQLb79bTA2a-"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "DEVICE = torch.device(\"cuda:0\")"
      ],
      "id": "tQLb79bTA2a-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f07bd89b"
      },
      "outputs": [],
      "source": [
        "# возьмем модель поменьше, так как дообучение это обновление весов\n",
        "model_name_or_path = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name_or_path, use_cache=False).to(DEVICE)"
      ],
      "id": "f07bd89b"
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "\n",
        "Небеса мои обетованные\n",
        "Что же вы молчите опять, высотою маня?\n",
        "Небеса мои обетованные\n",
        "Нелегко пред вами стоять, так услышьте меня!\n",
        "Солнцем разбужена\n",
        "И укрытая тоненьким кружевом\n",
        "Я вдыхаю её, забираю её\n",
        "Полусонную, в плен без оружия.\n",
        "И как без нее теперь?\n",
        "Как продлить эту краткую оттепель?\n",
        "Прижимаю сильней, да пребудет во мне\n",
        "Её запах и родинка на спине.\n",
        "Но время не вернуть, а счастье слепо\n",
        "Вижу два её осколка неба\n",
        "И теряются слова\n",
        "Синева-нева-нева-нева.\n",
        "Небеса мои обетованные\n",
        "Что же вы молчите опять, высотою маня?\n",
        "Небеса мои обетованные\n",
        "Нелегко пред вами стоять, так услышьте меня!\n",
        "Как все неправильно\n",
        "И за что полюбила она меня?\n",
        "Объяснения нет. Она послана мне\n",
        "За десяток веков ожидания.\n",
        "Солнцем разбужена\n",
        "И укрытая тоненьким кружевом\n",
        "Я вдыхаю её, забираю её\n",
        "Полусонную, в плен без оружия.\n",
        "Но время не вернуть, а счастье слепо\n",
        "Вижу два её осколка неба\n",
        "И теряются слова\n",
        "Синева-нева-нева-нева.\n",
        "Небеса мои обетованные\n",
        "Что же вы молчите опять, высотою маня?\n",
        "Небеса мои обетованные\n",
        "Нелегко пред вами стоять, так услышьте меня!\n",
        "Небеса мои обетованные\n",
        "Что же вы молчите опять, высотою маня?\n",
        "Небеса мои обетованные\n",
        "Нелегко пред вами стоять, так услышьте меня!\n",
        "Нелегко пред вами стоять, так услышьте меня!\n",
        "\n",
        "\"В первый день весны\n",
        "На краешке земли\n",
        "Нечаянно мы встретились с тобой\n",
        "Падал белый снег,\n",
        "И розы не цвели,\n",
        "Но к нам пришла весенняя любовь -\n",
        "Она была отчаянно красива\n",
        "В первый день зимы\n",
        "На краешке земли\n",
        "Нечаянно расстались мы с тобой\n",
        "Падал первый снег\n",
        "И розы отцвели –\n",
        "От нас ушла весенняя любовь\n",
        "Но ты была отчаянно красива\n",
        "Красиво ты вошла в мою грешную жизнь\n",
        "Красиво ты ушла из нее\n",
        "Но, играя, разбила мне душу\n",
        "А ведь это совсем не игрушка –\n",
        "Это сердце мое\n",
        "Красиво ты вошла в мою грешную жизнь\n",
        "Красиво ты ушла из нее\n",
        "Но, играя, разбила мне душу\n",
        "А ведь это совсем не игрушка –\n",
        "Это сердце мое\n",
        "Кончилась любовь\n",
        "Когда пришла зима\n",
        "Недолог был сезонный наш роман\n",
        "Но было все отчаянно красиво\n",
        "Красиво ты вошла в мою грешную жизнь\n",
        "Красиво ты ушла из нее\n",
        "Но, играя, разбила мне душу\n",
        "А ведь это совсем не игрушка –\n",
        "Это сердце мое\n",
        "Красиво ты вошла в мою грешную жизнь\n",
        "Красиво ты ушла из нее\n",
        "Но, играя, разбила мне душу\n",
        "А ведь это совсем не игрушка –\n",
        "Это сердце мое\n",
        "Это сердце мое\n",
        "\n",
        "Когда закончатся полеты первых ласточек\n",
        "И ты усталая придешь к себе домой\n",
        "Увидишь из окна слова из ярких лампочек\n",
        "Я напишу тебе: \"\"Не бойся, я с тобой\"\"\n",
        "Мы можем быть только на расстоянии и в невесомости\n",
        "Хочешь упасть – я неволить не стану, хочешь лететь – лети\n",
        "Но я тысячу раз обрывал провода\n",
        "Сам себе кричал: \"\"Ухожу навсегда\"\"\n",
        "Непонятно, как доживал до утра Салют, Вера!\n",
        "Но я буду с тобой или буду один\n",
        "Дальше не сбежать, ближе не подойти\n",
        "Прежде чем навек поменять номера Салют, Вера!\n",
        "Ты не сбываешься, хоть снишься в ночь на пятницу\n",
        "Не отзываешься ни на один пароль\n",
        "Не ошибаешься, и мне все чаще кажется\n",
        "Что ты посланница неведомых миров\n",
        "Мы можем быть только на расстоянии и в невесомости\n",
        "Хочешь упасть – я неволить не стану, хочешь лететь – лети\n",
        "Но я тысячу раз обрывал провода\n",
        "Сам себе кричал: \"\"Ухожу навсегда\"\"\n",
        "Непонятно, как доживал до утра Салют, Вера!\n",
        "Но я буду с тобой или буду один\n",
        "Дальше не сбежать, ближе не подойти\n",
        "Прежде чем навек поменять номера Салют, Вера!\n",
        "Но я тысячу раз обрывал провода\n",
        "Сам себе кричал: \"\"Ухожу навсегда\"\"\n",
        "Непонятно, как доживал до утра Салют, Вера!\n",
        "Но я буду с тобой или буду один\n",
        "Дальше не сбежать, ближе не подойти\n",
        "Прежде чем навек поменять номера Салют, Вера!\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "POBdZI8hEvRM"
      },
      "id": "POBdZI8hEvRM",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "\n",
        "# Сохраним обучающие данные в .txt файл \n",
        "train_path = 'train_dataset.txt'\n",
        "with open(train_path, \"w\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "# Создание датасета\n",
        "train_dataset = TextDataset( tokenizer=tokenizer,file_path=train_path,block_size=64, \n",
        "                            overwrite_cache=True)\n",
        "  \n",
        "# специальный класс который будет подавать в модель данные в нужном ей виде\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "Hma-oWHXFocQ"
      },
      "id": "Hma-oWHXFocQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments( \n",
        "    output_dir= \"./finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=100, \n",
        "    per_device_train_batch_size=32, \n",
        "    per_device_eval_batch_size=32,  \n",
        "    gradient_accumulation_steps=16, \n",
        "    )\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    optimizers = (torch.optim.AdamW(model.parameters(),lr=1e-5),None) # Optimizer and lr scheduler\n",
        ")"
      ],
      "metadata": {
        "id": "cCZv57BDFsIf"
      },
      "id": "cCZv57BDFsIf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "mn4s6h7WFwGW",
        "outputId": "24269ba7-9336-4e3a-ed7e-deaf6a3ffc33"
      },
      "id": "mn4s6h7WFwGW",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 15\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 100\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 00:28, Epoch 100/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=100, training_loss=0.07115036487579346, metrics={'train_runtime': 28.8952, 'train_samples_per_second': 51.912, 'train_steps_per_second': 3.461, 'total_flos': 48992256000000.0, 'train_loss': 0.07115036487579346, 'epoch': 100.0})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Конец весны\"\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model.generate(input_ids, \n",
        "                        do_sample=True,\n",
        "                        temperature=1.4,\n",
        "                        top_k=50,\n",
        "                        max_length=200,\n",
        "                        )\n",
        "\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnVu2UX5GEWz",
        "outputId": "5302154b-712d-4a7e-a35e-00c3615c99ae"
      },
      "id": "gnVu2UX5GEWz",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Конец весны\n",
            "Леонардо\n",
            "\n",
            "\n",
            "Красиво она погибла\n",
            "Красиво…\n",
            "И мне от этого становится немного легче\n",
            "Ведь она была не одна\n",
            "И теперь ко мне прижимается чье-то горячее, твердое, как гранит,\n",
            "Укрывает меня своей пушистой варежкой?\n",
            "Небеса мои обетованные\n",
            "Что же это вы вдруг разомчитесь и возвратитесь домой?\n",
            "Прочь, прочь из райских кущ\n",
            "Солнцем разбужена я!\n",
            "Небеса мои обетованные\n",
            "Что же вы разомчитесь, прилетите ближе\n",
            "И да хранят вас\n",
            "Дыхательные леса и ладанке моя\n",
            "Небеса мои обетованные\n",
            "Что же вы разомчитесь, полетите скорее\n",
            "И да хранят вас\n",
            "Лапочка Моя!\n",
            "Полусонную снишься все чаще и чаще\n",
            "Дальше и ближе, милее и нежней\n",
            "Красиво она погибла,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = model.generate(input_ids, do_sample=False, num_beams=10, max_length=60)\n",
        "\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)\n",
        "\n",
        "#просто повторил "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9d4LE9XGktS",
        "outputId": "e8eee23b-8132-4181-eee6-ebc0479dce55"
      },
      "id": "y9d4LE9XGktS",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Конец весны\n",
            "Небеса мои обетованные\n",
            "Что же вы молчите опять, высотою маня?\n",
            "Небеса мои обетованные\n",
            "Нелегко пред вами стоять, так услышьте меня!\n",
            "Небеса мои обетованные\n",
            "Неле\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = model.generate(input_ids, do_sample=True, temperature=2.0, max_length=50)\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)\n",
        "#кросиво..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQjjACK0Gre_",
        "outputId": "fc4e13ab-b9ac-4450-fef5-ed625475b72d"
      },
      "id": "YQjjACK0Gre_",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Конец весны\n",
            "Частые клипы с Машей \n",
            "\n",
            "Безумству\n",
            "Небу скучно\n",
            "Небеса выстелены\n",
            "Красиво быть не может\n",
            "А вот влюбиться – вот тут\n",
            "http://svpressa.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = model.generate(input_ids, do_sample=True, temperature=1.2, max_length=50)\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYh8GiPBHDIe",
        "outputId": "374e25bf-74f9-4ce9-a92a-da8f3310ede2"
      },
      "id": "OYh8GiPBHDIe",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Конец весны\n",
            "Небеса мои обетованные\n",
            "Нелегко пред вами стоять, так услышьте меня!\n",
            "Небеса мои обетованные\n",
            "Солнцем разбужена\n",
            "И укрытая тоненьким кружевом\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = model.generate(input_ids, \n",
        "                     do_sample=True,\n",
        "                     temperature=1.7,\n",
        "                     top_k=10,\n",
        "                     max_length=50,\n",
        "                    )\n",
        "\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)\n",
        "#сложно оценить адекватность нейро-поэзии, я не Борис Орехов, но хотя бы не повтор, а генерация"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8a8LzfyHM4u",
        "outputId": "f50f69da-e937-46d2-c70b-94262abd7461"
      },
      "id": "u8a8LzfyHM4u",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Конец весны\n",
            "\n",
            "Когда наступит весна\n",
            "Мы сможем встретиться снова?\n",
            "Пальцем ты не разжимая \n",
            "Кожу на теле моем смываешь\n",
            "Я – цветок нарциссы\n",
            "Полусонно я лежу и не верю\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae8437e8"
      },
      "source": [
        "### Задание  2 (2 балла)\n",
        "\n",
        "Ответьте на следующие вопросы:\n",
        "\n",
        "1) В каких статья были представлены GPT-1, GPT-2, GPT-3?\n",
        "\n",
        "2) Как собирался обучающий корпус для GPT-3? Каким образом создатели старались обеспечить высокое качество текстов в обучающей выборке?"
      ],
      "id": "ae8437e8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Были взяты данные, собранные в Common Crawl, после чего была проведена мягкая дедупликация. К этим же данным добавили датасет WebText, два корпуса книг и английскую википедию.\n",
        "\n",
        "GPT1 - https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
        "\n",
        "GPT2 - https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
        "\n",
        "GPT3 - https://arxiv.org/pdf/2005.14165.pdf"
      ],
      "metadata": {
        "id": "RXBfRWkGKayy"
      },
      "id": "RXBfRWkGKayy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Были взяты данные, собранные в Common Crawl, после чего была проведена мягкая дедупликация. К этим же данным добавили датасет WebText, два корпуса книг и английскую википедию.\n",
        "Для достижения высокого качества текста старались избежать того, чтобы модель видела тестовые тексты на этапе обучения."
      ],
      "metadata": {
        "id": "zqn0PSN0KvKT"
      },
      "id": "zqn0PSN0KvKT"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "homework_10.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}