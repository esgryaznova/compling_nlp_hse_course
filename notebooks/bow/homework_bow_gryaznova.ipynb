{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3705663",
   "metadata": {},
   "source": [
    "# Домашнее задание № 2. Мешок слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf72d19",
   "metadata": {},
   "source": [
    "## Задание 1 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a045e99",
   "metadata": {},
   "source": [
    "У векторайзеров в sklearn есть встроенная токенизация на регулярных выражениях. Найдите способо заменить её на кастомную токенизацию"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b4d453",
   "metadata": {},
   "source": [
    "Обучите векторайзер с дефолтной токенизацией и с токенизацией razdel.tokenize. Обучите классификатор с каждым из векторизаторов. Сравните метрики и выберете победителя. \n",
    "\n",
    "(в вашей тетрадке должен быть код обучения и все метрики; если вы сдаете в .py файлах то сохраните полученные метрики в отдельном файле или в комментариях)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "129c4d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "from razdel import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bbc2365",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01341538",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3df99cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(inplace=True)\n",
    "test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58492d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(train.comment)\n",
    "X_test = vectorizer.transform(test.comment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50bcb106",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.toxic.values\n",
    "y_test = test.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc68f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "razdel_tokenizer = lambda doc: [t.text for t in list(tokenize(doc))]\n",
    "razdel_vectorizer = CountVectorizer(tokenizer=razdel_tokenizer)\n",
    "X2 = razdel_vectorizer.fit_transform(train.comment)\n",
    "X2_test = razdel_vectorizer.transform(test.comment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f0d3ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=8, class_weight='balanced')\n",
    "dt.fit(X, y)\n",
    "pred = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a14ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt2 = DecisionTreeClassifier(max_depth=8, class_weight='balanced')\n",
    "dt2.fit(X2, y)\n",
    "pred2 = dt2.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65d732b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.58      0.67       951\n",
      "         1.0       0.47      0.72      0.57       491\n",
      "\n",
      "    accuracy                           0.63      1442\n",
      "   macro avg       0.63      0.65      0.62      1442\n",
      "weighted avg       0.69      0.63      0.64      1442\n",
      "\n",
      "\n",
      "\n",
      "razdel classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.72      0.74       951\n",
      "         1.0       0.52      0.58      0.55       491\n",
      "\n",
      "    accuracy                           0.67      1442\n",
      "   macro avg       0.64      0.65      0.65      1442\n",
      "weighted avg       0.68      0.67      0.68      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('default classifier:')\n",
    "print(classification_report(y_test, pred))\n",
    "print('\\n\\nrazdel classifier:')\n",
    "print(classification_report(y_test, pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0006fa9",
   "metadata": {},
   "source": [
    "Раздел работает лучше, чем дефолтный токенизатор, но разница незначительная."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffa9f76",
   "metadata": {},
   "source": [
    "## Задание 2 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f358949",
   "metadata": {},
   "source": [
    "Преобразуйте таблицу с абсолютными частотностями в семинарской тетрадке в таблицу с tfidf значениями. (Таблица - https://i.ibb.co/r5Nc2HC/abs-bow.jpg) Формула tfidf есть в семинаре на картнике с пояснениями на английском. \n",
    "Считать нужно в питоне. Формат итоговой таблицы может быть любым, главное, чтобы был код и можно было воспроизвести вычисления. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5b50abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = [[1, 1, 1, 0, 0, 0], \n",
    "         [1, 1, 1, 0, 0, 0], \n",
    "         [3, 0, 1, 1, 0, 0], \n",
    "         [1, 0, 0, 1, 1, 0], \n",
    "         [0, 0, 0, 0, 0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd1d04ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>я</th>\n",
       "      <th>ты</th>\n",
       "      <th>и</th>\n",
       "      <th>только</th>\n",
       "      <th>не</th>\n",
       "      <th>он</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>я и ты</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ты и я</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>я, я и только я</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>только не я</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>он</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 я  ты  и  только  не  он\n",
       "я и ты           1   1  1       0   0   0\n",
       "ты и я           1   1  1       0   0   0\n",
       "я, я и только я  3   0  1       1   0   0\n",
       "только не я      1   0  0       1   1   0\n",
       "он               0   0  0       0   0   1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.DataFrame(freqs, columns = ['я', 'ты', 'и', 'только', 'не', 'он'], index = ['я и ты','ты и я','я, я и только я', 'только не я', 'он'])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6fdf398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>я</th>\n",
       "      <th>ты</th>\n",
       "      <th>и</th>\n",
       "      <th>только</th>\n",
       "      <th>не</th>\n",
       "      <th>он</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>я и ты</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ты и я</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>я, я и только я</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>только не я</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>он</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        я        ты         и    только        не   он\n",
       "я и ты           0.333333  0.333333  0.333333  0.000000  0.000000  0.0\n",
       "ты и я           0.333333  0.333333  0.333333  0.000000  0.000000  0.0\n",
       "я, я и только я  0.600000  0.000000  0.200000  0.200000  0.000000  0.0\n",
       "только не я      0.333333  0.000000  0.000000  0.333333  0.333333  0.0\n",
       "он               0.000000  0.000000  0.000000  0.000000  0.000000  1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = table.apply(lambda x: x/ x.sum(), axis = 1)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c481d105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>я</th>\n",
       "      <th>ты</th>\n",
       "      <th>и</th>\n",
       "      <th>только</th>\n",
       "      <th>не</th>\n",
       "      <th>он</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>я и ты</th>\n",
       "      <td>0.032303</td>\n",
       "      <td>0.132647</td>\n",
       "      <td>0.07395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ты и я</th>\n",
       "      <td>0.032303</td>\n",
       "      <td>0.132647</td>\n",
       "      <td>0.07395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>я, я и только я</th>\n",
       "      <td>0.058146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04437</td>\n",
       "      <td>0.079588</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>только не я</th>\n",
       "      <td>0.032303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.132647</td>\n",
       "      <td>0.23299</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>он</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.69897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        я        ты        и    только       не       он\n",
       "я и ты           0.032303  0.132647  0.07395  0.000000  0.00000  0.00000\n",
       "ты и я           0.032303  0.132647  0.07395  0.000000  0.00000  0.00000\n",
       "я, я и только я  0.058146  0.000000  0.04437  0.079588  0.00000  0.00000\n",
       "только не я      0.032303  0.000000  0.00000  0.132647  0.23299  0.00000\n",
       "он               0.000000  0.000000  0.00000  0.000000  0.00000  0.69897"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = {}\n",
    "for col in table.columns:\n",
    "    N = len(table)\n",
    "    df = len(table[col][table[col]!=0])\n",
    "    series = tfidf[col].apply(lambda x: x*np.log10(N/df))\n",
    "    frame[col] = series\n",
    "result = pd.DataFrame(frame)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5bc8de",
   "metadata": {},
   "source": [
    "## Задание 3 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8961bbf",
   "metadata": {},
   "source": [
    "Обучите 2 любых разных классификатора из семинара. Предскажите токсичность для текстов из тестовой выборки (используйте одну и ту же выборку для обоих классификаторов) и найдите 10 самых токсичных для каждого из классификаторов. Сравните получаемые тексты - какие тексты совпадают, какие отличаются, правда ли тексты токсичные?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46681ef",
   "metadata": {},
   "source": [
    "Требования к классификаторам:   \n",
    "а) один должен использовать CountVectorizer, другой TfidfVectorizer  \n",
    "б) у векторазера должны быть вручную заданы как минимум 5 параметров  \n",
    "в) у классификатора должно быть задано вручную как минимум 2 параметра  \n",
    "г)  f1 мера каждого из классификаторов должна быть минимум 0.75  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "964841c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.1, shuffle=True)\n",
    "train.reset_index(inplace=True)\n",
    "test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a984c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1), lowercase=True, stop_words = None, encoding=\"utf-8\")\n",
    "X = vectorizer.fit_transform(train.comment)\n",
    "X_test = vectorizer.transform(test.comment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e3243b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.toxic.values\n",
    "y_test = test.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed77d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=0.1, class_weight='balanced', max_iter = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cd916f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X, y)\n",
    "preds_log = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eed67f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.84      0.88       965\n",
      "         1.0       0.73      0.84      0.78       477\n",
      "\n",
      "    accuracy                           0.84      1442\n",
      "   macro avg       0.82      0.84      0.83      1442\n",
      "weighted avg       0.85      0.84      0.85      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5637cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#второй токенизатор для второго классификатора\n",
    "vectorizer = TfidfVectorizer(analyzer='word', lowercase=True, min_df=2, max_df=0.3, max_features = 10000)\n",
    "X2 = vectorizer.fit_transform(train.comment)\n",
    "X2_test = vectorizer.transform(test.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5107b2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.97      0.90       965\n",
      "         1.0       0.91      0.64      0.75       477\n",
      "\n",
      "    accuracy                           0.86      1442\n",
      "   macro avg       0.88      0.80      0.82      1442\n",
      "weighted avg       0.87      0.86      0.85      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_NB = MultinomialNB(alpha = 0.5, fit_prior = True)\n",
    "clf_NB.fit(X2, y)\n",
    "preds_NB = clf_NB.predict(X2_test)\n",
    "\n",
    "print(classification_report(y_test, preds_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d78bf132",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = clf.predict_proba(X_test)\n",
    "probas_NB = clf_NB.predict_proba(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33c3ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_toxic(probas):\n",
    "    probas = [p[1] for p in probas]\n",
    "    test['probas'] = probas\n",
    "    res_df = test.sort_values(by = 'probas', ascending = False)[:10]\n",
    "    res_df = res_df.reset_index(drop=True)\n",
    "    for i in range(10):\n",
    "        print('toxic:', res_df.loc[i].toxic)\n",
    "        print('comment: ', res_df.loc[i].comment)\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ea9d1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer and LogisticRegression: \n",
      "toxic: 1.0\n",
      "comment:  Так ты не по теме гомофорса давай, а в общем. Нахуя ты машешь банхамером? Это не лично твоя доска - не нравится, как общаются некоторые аноны - пили свой сайт и там наводи собственные порядки. Сейчас все выглядит так, что ты встав с утра не с той ноги, начинаешь раздавать баны, увидев там некий намек на срач и пытаясь искусственно зарегулировать доску под какое-то свое виденье того, как доска должна выглядить. Системность следования правилам раздела, отсутствие избирательного отношения или фаворитизма по отношению к тому или иному форсу, - залог стабильного постинга на честной доске. Что ты несешь, шизофреник задроченный? Честная доска? Ты таблеток принял? Кто тебе вообще дал ограничивать право на форс других людей, к тому же относительно фентези сука. Это игра, здесь все выдуманное, але. И вообще, что ты имеешь против гомиков, говна кусок? Ты уже не стесняешься и решил открыто ссать всем обитателям tes в лицо? надоели и стали больно уж натужными, или форсед по-другому. Лично тебе? Повторюсь, создавай свой сайт. Поэтому аноны, упорствующие в гомофорсе, могут и будут забанены на средние сроки - неделю или две. Вообще охуел, чушок. Залетит такое быдло с вконтактов и пикабу, возьмет модераторку и давай из своих туповатых взглядов раздавать баны на право и на лево. Особо упорствующие могут попасть под пермач А я считаю, что тебя должны снять с этой доски. Абу, обрати внимание. Это ебанейший аутист. Таким вообще никакой власти нельзя доверять, это неполноценные люди - даже на доске про компьютерную игру. Совсем поехал, дружок.\n",
      "\n",
      "toxic: 1.0\n",
      "comment:  Что ты несешь? ты поехавший? Ты понимаешь чем к примеру физичиеские законы отличаются от теорий разной степени охуительности ? Закон гравитации работет на всей планете Земля независимо от твоих манятеорий. Пространство блядь- трехмерно. А ты утверждаешь что оно может быть , 5,6 и прочее мерным, но доказать и продемонстрировать мы не можем по этому представим... Ты ты тупой верун Есть невидимый мужик , доказать и подтвердить мы не можем но давайте поверим... Иди нахуй. Ты настолько туп что не понимаешь даже что время- это иллюзия.\n",
      "\n",
      "toxic: 1.0\n",
      "comment:  хорошо тут все ясно а теперь поясни мне за то что он кладет на репорты за шитпостинг и нерелейтед. айробусом дедом, лизон создали тред, карине создали тред, оляше создали тред, так какого хуя этим ебланам треды отдельные не создать? какого хуя ты тут чертила свой рот открываешь, тварь малолетняя, вот из за таких как ты уебанов все и скатилось в гавно, потому что черта этого никто не репортит, а такие говноеды как ты все сливают в парашу\n",
      "\n",
      "toxic: 1.0\n",
      "comment:  Какой же ты тупой, пиздец. Сегодня хуярят алкашей и ты рад, завтра будут хуярить за не такой шмот , и быть может ты тоже будешь рад, если, конечно таскаешь тот шмот. Послезавтра будут хуярить уже тебя, просто потому, что ты чем-то кому-то не понравился. Но ныть уже будет поздно.\n",
      "\n",
      "toxic: 1.0\n",
      "comment:  В очередной раз убеждаюсь, что двачеры редкостные говноеды и хуесосы. Ладно там гоблин скурвился, ватанскую дичь уже давно несёт, но блять евген один из немногих блоггеров за которым нет явных грешков и провалов. Но блять сосачерам неприятна что он за совок топит, вот в чём причина , бугурта здесь многих. ПРОКЛЯТЫЙ САВВОК АРРЯ ГРЯЗНОШТАННИКИ ПИДАРАСЫ АРРЯ. Какой же дегенерат стандартный двачер, ты даже не человек блять, ебаный биомусор, неспособный понять обзорчик с ютуба. Способен только бомбить с абсолютно непонятных вещей, который видишь только ты, будь то злые коммуняки или пидарахи. Весь тред скатили не в обсуждение его лучших работ и скандалов, в том что евген блять КОММУНЯКА уххх печёт. Действительно, придерживался бы современных взглядов креативной молодёжи и был бы популярен как светов или сисян. но стоит отдать должое, у сисяна тоже ролики годные Видимо евген действительно знает что делает, раз его деятельность так подрывает дегенератов с нулевой. Охуенно\n",
      "\n",
      "toxic: 0.0\n",
      "comment:  та ну, хуйня это все про джентельменство . меня в свое время эти брачные игрища так сильно заебали, что я вообще перестал париться и начал себя вести так, как и обычно. без всех этих ужимок и образа славного парня . без зазрения совести перед девушками курил, харкался, матерился как сапожник, рассказывал охуитительные истории о том, как когда-то обосрался бухал ебал кого-то проснулся с обблеванной тян пересказывал пасты про говно и т.д. т.е вел себя как быдлан, как обычно я себя и веду, когда вокруг никого нет. и тогда девушки начали тянуться. после этого я сделал для себя вывод, что девушки вообще не воспринимают что ты им говоришь, что делаешь и т.д. они воспринимают лишь то, как ты все это говоришь и делаешь. когда я начал вести себя как обычно, я делал это, блять, уверенно, самовлюбленно, нагло и аки самец . может это будило в девушках какие-то там инстинкты и они думали что-то в стиле ну ничего что быдло, зато какой уверенный, ВАХ просто! или я хуй знает, но такое поведение позволило мне пользоваться определенной популярностью у девушек. вообще если покопаться во всей этой теме, образ отношений, любви, поведения во всех этих брачных игрищах, все это навязывается нам с рождения источниками массовой информации, через книги, фильмы, красивые любовные истории и т.д. веди себя как джентельмен, жди ту, которая встрепетнет твою душу и от которой твое сердце забьется чаще, а в животе будут летать бабочки, положи всю свою жизнь на счастье второй половинки, будь для нее горой, добытчиком, ебырем-террористом и вся эта прочая романтичная, сопливая хуета. и все это в корне не верно. реальная любовь и отношения, не имеют ничего общего с тем, что нам с рождения вбивают в голову. все эти высокие чувства - обычное желание ебаться. видишь человека - получаешь дозу гормона - кайфуешь. не видишь человека - нет дозы гормона - появляется ломка - ищешь способы увидеть человека. вот тебе описание любви. через какое-то время эти чувства угаснут так или иначе и если у вас кроме этих чуйств нихуя не было, то развод и девичья фамилия. настоящая любовь, это когда у вас есть тысячи причин быть вместе, помимо того, что у вас стойкое желание ебаться. даже когда желание ебаться уже прошло, вам все равно приятно находиться вместе. такое же навязывание светлой и романтишной любви , на моей памяти, вызывало не мало проблем у моих знакомых девушек. они втупую сидели и ждали принца на белом коне ГОДАМИ, ссылаясь на охуенные фразочки если твое, то ты не пропустишь , обещанного три года ждут и прочую ебанину. я и сам к некоторым пытался лезть как кандидат в парни, ухаживал, звал гулять, пытался побольше времени проводить с ними, но все упиралось в одну хуйню. а именно вот есть парень, он очень популярен, он вообще не мой уровень, у него целые армии из девушек желающих попрыгать на его куканчике, и вот она я вся такая красивая, посижу на попе смирно, нихуя не буду делать, он обязательно меня заметит, а пока что я буду отметать кучу нормальных вариантов отношений! ох бедная я несчастная, парня найти то себе не могу, 25 лет а все в девках хожу! . а все почему? а потому что им с детства вбивали, что ухаживать должны мальчики, подкатывать должны мальчики, а ты, вся такая красивая, должна сидеть и ждать пока к тебе подкатят и лишь отсеивать нормальных парней в ожидании его - ПРЫНЦА НА БЕЛОМ КОНЕ!!1 именно так любовь и работает, нахуй!!11! чето меня занесло и печет пиздец. все это джентельменство хуйня собачья. образ современной любви - та еще дичь. надеюсь через пару десятков поколений, весь этот романтизм и брачные игрища станут архаизмом и люди просто будут влюбляться и быть вместе, без всего этого социально-любовного шлака.\n",
      "\n",
      "toxic: 1.0\n",
      "comment:  Э че бля а? Ты че нах бл иди сюда епты.. Ты за царя гнида? Я тя щас грязь растопчу хуесос ептыбля\n",
      "\n",
      "toxic: 1.0\n",
      "comment:  - А вот тебе грамота, дядя. - В очко себе свою грамоту засуньте, плашмя причём, не сворачивая. - Ах так, тогда мы всем расскажем, что ты нарушитель, мы тебя даже подозревали, что ты бухой за рулем ездил. - На хую вертел я ваши подозрения, врачи сказали, что я чист.\n",
      "\n",
      "toxic: 1.0\n",
      "comment:  Ты-то пиздец сильный, да? Лол. Нет, мой дорогой ебанашка, ты должен знать, что миром правят не сильные, а умные, хитрые и коммуникабельные. Сильные на стройках и в шахтах ебашат, за скромный прайс. Что ж касается этого уебана, то довольно скоро он нарвется на рили отморозка, которому похуй будет на да это пранк, чувак . Пилю прохладную: один мой знакомый ночью бухал с пацыками, че-то дурачились они, гуляли ночью по ДС. Решили на светофоре встать вчетвером, типа они в невидимой тачке. Я рядом чурка на приоре. Они че-то с чуркой давай в гляделки играть, типа гонки на светофоре . Чурка юмор не осилил, вышел и порезал моего приятеля и его кореша ножом. Один помер в больничке, мой знакомый остался жив, но че-то уме удалили и все пузо перешито теперь.\n",
      "\n",
      "toxic: 1.0\n",
      "comment:  Я живу вопреки стараниям твоего хероя, а ты, видимо, глуп и не думал никогда о том, что заслуга вашего хероя - это смерти многих и многих людей от стараний его и его кровавой прислуги, которые не родили детей и не продолжили свои роды, а ты коптишь пикабу и лижешь ему задницу, хотя она уже давно сгнила и черти жарят его душу в аду. Мой прадедушка был раскулачен за дом-пятистенок, ты знаешь что такое дом-пятистенок для семьи где восемь детей? И жили они в землянке, которую сами выкопали. Ты мне никогда не докажешь что это говно что-то стоящее сделало для меня, можешь не усираться.\n",
      "\n",
      "TfidfVectorizer and MultinomialNB: \n",
      "toxic: 1.0\n",
      "comment:  ИДИ НАХУЙ, СУКА, ЗАВАЛИ ЕБАЛО, БЛЯТЬ\n",
      "\n",
      "toxic: 1.0\n",
      "comment:  Перепутали в роддоме негритенка, хохла и русского. Хохол такой заходит в палату и кричит Слава Украине! Негритенок засмеялся, а русский и хохленок обосрались и как давай говном кидаться.\n",
      "\n",
      "toxic: 1.0\n",
      "comment:  Э че бля а? Ты че нах бл иди сюда епты.. Ты за царя гнида? Я тя щас грязь растопчу хуесос ептыбля\n",
      "\n",
      "toxic: 1.0\n",
      "comment:  В очередной раз убеждаюсь, что двачеры редкостные говноеды и хуесосы. Ладно там гоблин скурвился, ватанскую дичь уже давно несёт, но блять евген один из немногих блоггеров за которым нет явных грешков и провалов. Но блять сосачерам неприятна что он за совок топит, вот в чём причина , бугурта здесь многих. ПРОКЛЯТЫЙ САВВОК АРРЯ ГРЯЗНОШТАННИКИ ПИДАРАСЫ АРРЯ. Какой же дегенерат стандартный двачер, ты даже не человек блять, ебаный биомусор, неспособный понять обзорчик с ютуба. Способен только бомбить с абсолютно непонятных вещей, который видишь только ты, будь то злые коммуняки или пидарахи. Весь тред скатили не в обсуждение его лучших работ и скандалов, в том что евген блять КОММУНЯКА уххх печёт. Действительно, придерживался бы современных взглядов креативной молодёжи и был бы популярен как светов или сисян. но стоит отдать должое, у сисяна тоже ролики годные Видимо евген действительно знает что делает, раз его деятельность так подрывает дегенератов с нулевой. Охуенно\n",
      "\n",
      "toxic: 1.0\n",
      "comment:  Сука, ненавижу тебя. Ты заставил меня плакать\n",
      "\n",
      "toxic: 1.0\n",
      "comment:  Какой нахуй шок, что ты выдумал уебок, может тебя ещё и за цп не забанят в твоем маня-шок треде? Скоро будешь бан ловить за свои скримеры, сучье отродье .\n",
      "\n",
      "toxic: 1.0\n",
      "comment:  Так ты не по теме гомофорса давай, а в общем. Нахуя ты машешь банхамером? Это не лично твоя доска - не нравится, как общаются некоторые аноны - пили свой сайт и там наводи собственные порядки. Сейчас все выглядит так, что ты встав с утра не с той ноги, начинаешь раздавать баны, увидев там некий намек на срач и пытаясь искусственно зарегулировать доску под какое-то свое виденье того, как доска должна выглядить. Системность следования правилам раздела, отсутствие избирательного отношения или фаворитизма по отношению к тому или иному форсу, - залог стабильного постинга на честной доске. Что ты несешь, шизофреник задроченный? Честная доска? Ты таблеток принял? Кто тебе вообще дал ограничивать право на форс других людей, к тому же относительно фентези сука. Это игра, здесь все выдуманное, але. И вообще, что ты имеешь против гомиков, говна кусок? Ты уже не стесняешься и решил открыто ссать всем обитателям tes в лицо? надоели и стали больно уж натужными, или форсед по-другому. Лично тебе? Повторюсь, создавай свой сайт. Поэтому аноны, упорствующие в гомофорсе, могут и будут забанены на средние сроки - неделю или две. Вообще охуел, чушок. Залетит такое быдло с вконтактов и пикабу, возьмет модераторку и давай из своих туповатых взглядов раздавать баны на право и на лево. Особо упорствующие могут попасть под пермач А я считаю, что тебя должны снять с этой доски. Абу, обрати внимание. Это ебанейший аутист. Таким вообще никакой власти нельзя доверять, это неполноценные люди - даже на доске про компьютерную игру. Совсем поехал, дружок.\n",
      "\n",
      "toxic: 1.0\n",
      "comment:  Будешь орать когда тебя десять хачей в жопу ебать начнут, сжв-хуесос. Хотя наверняка тебе понравится.\n",
      "\n",
      "toxic: 1.0\n",
      "comment:  ТРЕД РУССКОГО НАУЧПОП КОНТЕНТА УБЕРИ НАХУЙ СВОИХ ПЕТУХОВ ОТСЮДА, ШКОЛЬНИК ЕБАНЫЙ\n",
      "\n",
      "toxic: 1.0\n",
      "comment:  хорошо тут все ясно а теперь поясни мне за то что он кладет на репорты за шитпостинг и нерелейтед. айробусом дедом, лизон создали тред, карине создали тред, оляше создали тред, так какого хуя этим ебланам треды отдельные не создать? какого хуя ты тут чертила свой рот открываешь, тварь малолетняя, вот из за таких как ты уебанов все и скатилось в гавно, потому что черта этого никто не репортит, а такие говноеды как ты все сливают в парашу\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-f925beb516d6>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['probas'] = probas\n"
     ]
    }
   ],
   "source": [
    "print('CountVectorizer and LogisticRegression: ')\n",
    "clf1 = find_toxic(probas)\n",
    "print('TfidfVectorizer and MultinomialNB: ')\n",
    "clf2 = find_toxic(probas_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef6b6877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic_x</th>\n",
       "      <th>probas_x</th>\n",
       "      <th>index_y</th>\n",
       "      <th>toxic_y</th>\n",
       "      <th>probas_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>439</td>\n",
       "      <td>Так ты не по теме гомофорса давай, а в общем. ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>439</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.985885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>241</td>\n",
       "      <td>хорошо тут все ясно а теперь поясни мне за то ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999938</td>\n",
       "      <td>241</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>380</td>\n",
       "      <td>В очередной раз убеждаюсь, что двачеры редкост...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998808</td>\n",
       "      <td>380</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.987138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96</td>\n",
       "      <td>Э че бля а? Ты че нах бл иди сюда епты.. Ты за...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.997980</td>\n",
       "      <td>96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.989602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index_x                                            comment  toxic_x  \\\n",
       "0      439  Так ты не по теме гомофорса давай, а в общем. ...      1.0   \n",
       "1      241  хорошо тут все ясно а теперь поясни мне за то ...      1.0   \n",
       "2      380  В очередной раз убеждаюсь, что двачеры редкост...      1.0   \n",
       "3       96  Э че бля а? Ты че нах бл иди сюда епты.. Ты за...      1.0   \n",
       "\n",
       "   probas_x  index_y  toxic_y  probas_y  \n",
       "0  1.000000      439      1.0  0.985885  \n",
       "1  0.999938      241      1.0  0.982721  \n",
       "2  0.998808      380      1.0  0.987138  \n",
       "3  0.997980       96      1.0  0.989602  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.merge(clf2, how = 'inner', on ='comment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d9038",
   "metadata": {},
   "source": [
    "Тексты правда очень таксичные....совпали только 4 комментария. В первом классификаторе тексте подлиннее оказались. Хотя совпали 2 очень длинных, 1 средний и 1 короткий, так что, скорее всего, никакого интересного наблюдения тут нет."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68324753",
   "metadata": {},
   "source": [
    "## *Задание 4 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7794f97",
   "metadata": {},
   "source": [
    "Для классификаторов LogisticRegression, Decision Trees, Naive Bayes, Random Forest найдите способ извлечь важность признаков для предсказания токсичного класса. Сопоставьте полученные числа со словами (или нграммами) в словаре и найдите топ - 5 \"токсичных\" слов для каждого из классификаторов. \n",
    "\n",
    "Важное требование: в топе не должно быть стоп-слов. Для этого вам нужно будет правильным образом настроить векторизацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7d8f312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86d147ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gryaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ab1ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8745c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordListCorpusReader in 'C:\\\\Users\\\\gryaz\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eaf9062a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "хохлы\n",
      "хохлов\n",
      "тебе\n",
      "дебил\n",
      "нахуй\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(data, test_size=0.1, random_state = 100)\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "y = train.toxic.values\n",
    "y_test = test.toxic.values\n",
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1), lowercase=True, stop_words=stopwords.words('russian'))\n",
    "X = vectorizer.fit_transform(train.comment)\n",
    "X_test = vectorizer.transform(test.comment) \n",
    "clf = LogisticRegression(C=0.1, class_weight='balanced')\n",
    "clf.fit(X, y)\n",
    "importance = clf.coef_\n",
    "words ={value:key for key, value in vectorizer.vocabulary_.items()}\n",
    "idx = importance.argsort()\n",
    "for i in idx[0][:-6:-1]:\n",
    "    print(words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1621065d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тебе\n",
      "хохлы\n",
      "очень\n",
      "нахуй\n",
      "хохлов\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1), lowercase=True, stop_words=stopwords.words('russian'))\n",
    "X = vectorizer.fit_transform(train.comment)\n",
    "X_test = vectorizer.transform(test.comment) \n",
    "clf = DecisionTreeClassifier(max_depth=8, class_weight='balanced')\n",
    "clf.fit(X, y)\n",
    "importance = clf.feature_importances_\n",
    "words ={value:key for key, value in vectorizer.vocabulary_.items()}\n",
    "idx = importance.argsort()\n",
    "for i in idx[:-6:-1]:\n",
    "    print(words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac09ca93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "это\n",
      "просто\n",
      "тебе\n",
      "всё\n",
      "вообще\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gryaz\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:101: FutureWarning: Attribute coef_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1), lowercase=True, stop_words=stopwords.words('russian'))\n",
    "X = vectorizer.fit_transform(train.comment)\n",
    "X_test = vectorizer.transform(test.comment) \n",
    "clf = MultinomialNB(alpha = 0.5, fit_prior = True)\n",
    "clf.fit(X, y)\n",
    "importance = clf.coef_\n",
    "words ={value:key for key, value in vectorizer.vocabulary_.items()}\n",
    "idx = importance.argsort()\n",
    "for i in idx[0][:-6:-1]:\n",
    "    print(words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d91faee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тебе\n",
      "хохлы\n",
      "блядь\n",
      "быдло\n",
      "тред\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1), lowercase=True, stop_words=stopwords.words('russian'))\n",
    "X = vectorizer.fit_transform(train.comment)\n",
    "X_test = vectorizer.transform(test.comment) \n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=20)\n",
    "clf.fit(X, y)\n",
    "importance = clf.feature_importances_\n",
    "words ={value:key for key, value in vectorizer.vocabulary_.items()}\n",
    "idx = importance.argsort()\n",
    "for i in idx[:-6:-1]:\n",
    "    print(words[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd560c4",
   "metadata": {},
   "source": [
    "Практически во всех топах, кроме Наивного Байеса, присутствуют словоформы лексему хохлы, хорошо бы предварительно тексты лемматизировать. И можно еще убрать дополнительно слова \"это\", \"всё\", \"просто\" и т.д., которые как раз вылезли в наивном байесе почему-то."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
