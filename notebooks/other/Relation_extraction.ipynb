{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Извлечение отношений\n",
    "\n",
    "Традиционно извлечение отношений решается как задача классификации. Нужно связать именованные сущности в предложении какими-то заранее известными типами связей. Чаще всего это отношения вроде work_at, born_in, located_in, head_of. В биомедициских текстах извлечение отношений применяется для извлечения взаимодействия белков и поиска пар (лекарство, болезнь). Количество аргументов вообще может быть любым, но чаще всего ограничиваются бинарными отношениями (субъект, предикат, объект). \n",
    "\n",
    "Посмотрим как это работает на размеченном датасете.\n",
    "\n",
    "Данные - https://github.com/thunlp/FewRel/blob/master/data/train.json\n",
    "\n",
    "\n",
    "FewRel - это датасет для few-shot обучения. Это немного другая задача, для которой нужна нейронная сеть с определенной архитектурой и тест сет тут состоит из отношений других типов. Подробнее про задачу и датасет можно почитать вот тут - https://arxiv.org/pdf/1810.10147v1.pdf.\n",
    "\n",
    "Мы его будем использовать для обычной классификации. Оценивать качество будем на кросс-валидации.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from scipy.sparse import hstack\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('train.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В датасете 64 типа отношений и у каждого типа 700 - предложений, в которых оно встречается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Типы отношений обознчаются как-то условно, но в статье про датасет можно посмотреть соответствие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter([len(data[k]) for k in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый инстанс - это предложения и разметка. Ключ h - это главное слово, t - зависимое. Предложения разделены на токены и в разметке указаны слова, уникальный номер и индексы сущности в предложении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['P127'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения нам нужно каким-то образом перевести такую разметку в один вектор и сопоставить ему тип отношения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как это сделать? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стандартный способ - достать контекст слева от первой сущности, между сущностями и после второй сущности. Левые и правые контексты можно ограничить каким-то числом (например, 3 слова). Для каждого контекста можно получить вектор обычными способами - например через TfidfVectorizer. Потом эти вектора конкатенируются в один."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё в этот вектор можно добавить длину контекста, тэги сущностей, сами сущности, порядок сущностей и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# тут будем держать сущности\n",
    "ent1 = []\n",
    "ent2 = []\n",
    "\n",
    "# тут будем хранить контексты\n",
    "left = []\n",
    "right = []\n",
    "middle = []\n",
    "\n",
    "# целые предложения тоже на всякий случай достанем\n",
    "sents = []\n",
    "\n",
    "# целевая переменная (тип отношений будет тут)\n",
    "target = []\n",
    "\n",
    "\n",
    "# проходим по типам отношений\n",
    "for key in data:\n",
    "    # по каждому инстансу\n",
    "    for instance in data[key]:\n",
    "        \n",
    "        tokens = instance['tokens']\n",
    "        sents.append(tokens)\n",
    "        \n",
    "        ent1.append(' '.join([tokens[i] for i in instance['h'][2][0]]))\n",
    "        ent2.append(' '.join([tokens[i] for i in instance['t'][2][0]]))\n",
    "        \n",
    "        \n",
    "        # h и t не обязательно идут в таком порядке\n",
    "        # чтобы достать контексты нужно понять что из них идет первым\n",
    "        if instance['h'][2][0][0] < instance['t'][2][0][0]:\n",
    "            first, second = 'h', 't'\n",
    "        else:\n",
    "            second, first = 'h', 't'\n",
    "        \n",
    "        \n",
    "        # индексы сущностей\n",
    "        first_start = instance[first][2][0][0]\n",
    "        first_end = instance[first][2][0][-1]\n",
    "        second_start = instance[second][2][0][0]\n",
    "        second_end = instance[second][2][0][-1]\n",
    "\n",
    "        # левый контекст - это три слова слева от начала первой сущности\n",
    "        # если слева меньше 3 слов, то добавим тэгов <START>\n",
    "        left_context = tokens[max(0, first_start-3):first_start]\n",
    "        left.append((['<START>']*(3-len(left_context))) + left_context)\n",
    "        \n",
    "        # правый контекст - это 3 слова после последнего слова второй сущности\n",
    "        # если справа меньше 3 слов, то добавим тэгов <END>\n",
    "        right_context = tokens[second_end+1:second_end+4]\n",
    "        right.append(right_context + (['<END>']*(3-len(right_context))))\n",
    "\n",
    "        # средний контекст - это слова между последний словом первой сущности \n",
    "        # и первым словом второй сущности\n",
    "        middle_context = tokens[first_end+1:second_start]\n",
    "        middle.append(middle_context)\n",
    "\n",
    "        target.append(key)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для тфидф векторайзера склеим токены. Обучим один общий векторайзер на всех текстах, но можно для каждого контекста обучить свой. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lefts_s = [' '.join(l) for l in left]\n",
    "rights_s = [' '.join(l) for l in right]\n",
    "middles_s = [' '.join(l) for l in middle]\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=3000, ngram_range=(1,2))\n",
    "tfidf.fit(lefts_s + rights_s + middles_s)\n",
    "\n",
    "l = tfidf.fit_transform(lefts_s)\n",
    "r = tfidf.fit_transform(rights_s)\n",
    "m = tfidf.fit_transform(middles_s)\n",
    "\n",
    "X = csr_matrix(hstack([l,m,r])) # чтобы можно было по индексам доставать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим качество на StratifiedKFold. Посчитаем стандартные метрики (c микро и макро усреднением). Ещё сделаем матрицу ошибок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "skf = StratifiedKFold(n_splits=N, shuffle=True)\n",
    "metrics_macro = np.zeros((3))\n",
    "metrics_micro = np.zeros((3))\n",
    "conf = np.zeros((len(set(y)), len(set(y))))\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # Можно конечно что-нибудь посложнее, но для примера хватит и логрега\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X[train_index], y[train_index])\n",
    "    preds = clf.predict(X[test_index])\n",
    "    \n",
    "    metrics_macro += precision_recall_fscore_support(y[test_index], preds, average='macro')[:3]\n",
    "    metrics_micro += precision_recall_fscore_support(y[test_index], preds, average='micro')[:3]\n",
    "    \n",
    "    conf += confusion_matrix(y[test_index], preds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics_micro/N)\n",
    "print(metrics_macro/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "sns.heatmap(data=np.round(conf/3).astype(int), \n",
    "            annot=True, \n",
    "            fmt=\"d\", xticklabels=clf.classes_, yticklabels=clf.classes_, ax=ax)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество получается не очень хорошее. Но для такой сложной задачи (и не очень подходящего датасета) - это нормально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это качество ещё сильнее ухудшится, если мы захотим применить классификатор на неразмеченных текстах. В этом датасете нам сразу даны разметки именованных сущностей. В реальной задаче - их нужно находить самим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим как это работает на каких-нибудь новостных текстах. Именованные сущности достанем с помощью spacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Новостные тексты взяты отсюда - https://webhose.io/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = json.load(open('news/'+files[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут полно метаинформации, но нам нужны только тексты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(doc['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy сразу достает сущности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.string, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут много разных типов сущностей. Возьмем только PERSON и ORG (GPE тоже скорее организация, поэтому включим её тоже)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нас интересуют только предложения, в которых есть хотя бы одна персона и одна организация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sents_with_ents = []\n",
    "\n",
    "for file in files:\n",
    "    js = json.load(open('news/'+file))\n",
    "    text = js['text']\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        sent = sent.as_doc()\n",
    "        pers = []\n",
    "        orgs = []\n",
    "        for entity in sent.ents:\n",
    "            if entity.label_ == 'PERSON':\n",
    "                pers.append(entity)\n",
    "            elif entity.label_ in ['ORG', 'GPE']:\n",
    "                orgs.append(entity)\n",
    "        if pers and orgs:\n",
    "            sents_with_ents.append((sent, pers, orgs))\n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь точно также извлечем контексты вокруг сущностей. Единственное отличие - нам нужно учитывать случаи, когда в одном предложении сразу несколько сущностей. Просто переберем все сочетания персон с организациями. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lefts = []\n",
    "ents1 = []\n",
    "ents2 = []\n",
    "rights = []\n",
    "middles = []\n",
    "orders = []\n",
    "sents = []\n",
    "tags = []\n",
    "for sent, pers, orgs in sents_with_ents:\n",
    "    for per in pers:\n",
    "        # пары с этой персоной и остальными организациями\n",
    "        pairs = [[per, org] for org in orgs]\n",
    "        \n",
    "        sent = list(sent)\n",
    "        \n",
    "        for ent1, ent2 in pairs:\n",
    "            sents.append(' '.join([str(w) for w in sent]))\n",
    "            if ent1.start > ent2.start:\n",
    "                ent2, ent1 = ent1, ent2\n",
    "\n",
    "            # ent.start в spacy совпадает с начальным токеном\n",
    "            # а вот ent.end - это номер последнего токена + 1\n",
    "            # поэтому этот кусок немного отличается\n",
    "            left_context = sent[max(0, ent1.start-3):ent1.start]\n",
    "            lefts.append(['<START>']*(3-len(left_context)) + left_context)\n",
    "            \n",
    "            right_context = sent[ent2.end:ent2.end+3]\n",
    "            rights.append(right_context + (['<END>']*(3-len(right_context))))\n",
    "            \n",
    "            middles.append(sent[ent1.end:ent2.start])\n",
    "            \n",
    "            ents1.append(ent1.string)\n",
    "            ents2.append(ent2.string)\n",
    "            \n",
    "            tags.append([ent1.label_, ent2.label_])\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lefts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим какие вообще пары сущностей достались."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entpair = ['#'.join(e) for e in zip(ents1, ents2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(entpair).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему-то выделяются новые строки. Но попробуем с ними."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lefts_s = [' '.join([str(x) for x in l]) for l in lefts]\n",
    "rights_s = [' '.join([str(x) for x in l]) for l in rights]\n",
    "middles_s = [' '.join([str(x) for x in l]) for l in middles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lefts_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = TfidfVectorizer(max_features=3000)\n",
    "# tfidf.fit(lefts_s + rights_s + middles_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = tfidf.transform(lefts_s)\n",
    "r = tfidf.transform(rights_s)\n",
    "m = tfidf.transform(middles_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = hstack([l,m,r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим на то, как предсказывается какой-нибудь класс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(clf.classes_).index('P118')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict_proba(X_)[:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(np.array(sents)[pred > 0.3], np.array(entpair)[pred > 0.3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тема вроде бы распознается, но отношения выделяются не очень хорошо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно попробовать кластеризовать предложения. Вдруг какие-то отношения выделятся в кластер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = MiniBatchKMeans(100, verbose=1, reassignment_ratio=0.3, max_no_improvement=500)\n",
    "cluster.fit(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl2id = defaultdict(list)\n",
    "\n",
    "for i, cl in enumerate(cluster.labels_):\n",
    "    cl2id[cl].append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entpair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('clusters.txt', 'w')\n",
    "\n",
    "for cl in cl2id:\n",
    "    f.write('CLUSTER __' + str(cl) + '__\\n')\n",
    "    for i in cl2id[cl]:\n",
    "        f.write(entpair[i].replace('\\n', ' ') + '\\n')\n",
    "        f.write(sents[i].replace('\\n', ' ') + '\\n')\n",
    "    f.write('\\n\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Другие подходы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для нейросетей задачу можно представить как seq2seq - каждому токену соответствует тэг H, T или O (можно добавить Begin, Inside тэги, чтобы отметить многословные сущности). И можно вообще решать задачу извлечения сущностей и отношений вместе. Про это читать,\n",
    "например, тут:  https://www.semanticscholar.org/paper/Joint-learning-of-named-entity-recognition-and-Xu-Li/31ce449618068343f9f83c904c7fd062ba943c8e?navId=references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извлечение отношений часто пытаются решать без учителя. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из подходов - **bootstrapping**, о котором мы говорили в прошлый раз. Можно выбрать какой-то набор пар сущностей, которые выражают отношение и найти предложения, в которых эта пара сущностей встречается. Потом найти похожие по контекстам предложения и считать их представителями этого класса. Затем можно достать пары сущностей, в которых они употреблены и повторить все заново. \n",
    "\n",
    "Про это можно почитать вот тут:\n",
    "\n",
    "1) одна из первых работ - https://pdfs.semanticscholar.org/6f16/7cce628ec4983788ddf21587630afebf43ce.pdf?_ga=2.136426931.2051797770.1542970757-1216332217.1520769589 (от создателя гугла)\n",
    "\n",
    "2) https://pdfs.semanticscholar.org/189e/d3f749766d02d42eb5b6d71017e085c212d4.pdf?_ga=2.112375175.2051797770.1542970757-1216332217.1520769589\n",
    "\n",
    "3) Тут бустраппинг делается с помощью word2vec -https://pdfs.semanticscholar.org/fe6e/56ec0a1f5d673a4ab22e716f2c846b497f9c.pdf?_ga=2.179043751.2051797770.1542970757-1216332217.1520769589\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другой популярный метод - **distant supervision**. Идея очень похожая, только вместо того, чтобы самим придумывать положительные примеры - их берут из какой-нибудь базы данных. Например, из DBPedia или из Freebase. Достав большое количество упоминаний, можно собрать уже достаточно большую обучающую выборку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почитать можно тут:  \n",
    "1) первая статья по теме (от журафского) https://www.semanticscholar.org/paper/Distant-supervision-for-relation-extraction-without-Mintz-Bills/8f8139b63a2fc0b3ae8413acaef47acd35a356e0  \n",
    "    \n",
    "2) тут предлагаются методы убрать шум из такой разметки - https://www.semanticscholar.org/paper/Denoising-Distant-Supervision-for-Relation-via-Han-Liu/3d13ee24493a6c2a0477b15e5145ba5868c3df40 \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё одно большое направление - Open Information Extraction. Идея тут в том, чтобы извлекать из предложений (или текстов) все отношения в виде троек (субъект, предикат, объект). \n",
    "\n",
    "Например, из предложения __The U.S. president Barack Obama gave his speech on Tuesday and Wednesday to thousands of people.__ излекутся тройки:  \n",
    "\n",
    "__(Barack Obama, president, U.S)  \n",
    "(Barack Obama, gave, his speech)  \n",
    "(Barack Obama, gave his speech, on Tuesday)  \n",
    "(Barack Obama, gave his speech, on Wednesday)  \n",
    "(Barack Obama, gave his speech, to thousands of people)__\n",
    "\n",
    "Таким образом можно извлекать неограниченное количество типов отношений, т.е. не нужно для каждого типа размечать предложения. Однако появляется необходимость каким-то образом кластеризовать разные способы выражения одного типа отношений. \n",
    "\n",
    "Почитать про OpenIE можно тут:\n",
    "\n",
    "1) оригинальная работа http://www.aaai.org/Papers/IJCAI/2007/IJCAI07-429.pdf  \n",
    "2) одна из самых известных работ http://ml.cs.washington.edu/www/media/papers/reverb_emnlp2011.pdf  \n",
    "3) одна из последних статей http://www.cse.iitd.ac.in/~mausam/papers/coling18.pdf  \n",
    "\n",
    "Реализация OpenIE есть StandfordNLP и вот тут - https://github.com/dair-iitd/OpenIE-standalone"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
