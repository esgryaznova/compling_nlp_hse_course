{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic role labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важные ресурсы, связанные с srl:  \n",
    "\n",
    "    Verbnet - https://verbs.colorado.edu/~mpalmer/projects/verbnet.html  \n",
    "    Propbank - https://propbank.github.io/  \n",
    "    Framenet - https://framenet.icsi.berkeley.edu/  \n",
    "    Framebank (на русском) - https://github.com/olesar/framebank  \n",
    "    Conll dataset - http://www.lsi.upc.edu/~srlconll/  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению они либо под лицензией (нужно что-то заполнять или даже платить), либо так сложно устроены, что непонятно как с ними работать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому в семинаре мы будем использовать данные отсюда - https://dada.cs.washington.edu/qasrl/\n",
    "\n",
    "Это нестандартная коллекция. Тут вместо отметок ролей - вопросы. Это может показаться неправильным, но если подумать, то семантические роли - это как раз ответы на вопросы \"кто/что, когда, как, почему\". \n",
    "\n",
    "Формулировка задачи в вопросно-ответной форме вообще очень актуальная вещь. В saleforce даже сделали нейронку, которая может решать сразу несколько разных задач, сформулированных как вопросы-ответы. Этот датасет там тоже есть - https://decanlp.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('wiki1.train.qa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формат тут неочевидный, поэтому проще почитать документацию - https://dada.cs.washington.edu/qasrl/data/readme.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала попробуем распозновать только предикаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAIN SET\n",
    "\n",
    "sent_pred_pairs_train = defaultdict(list)\n",
    "data = open('wiki1.train.qa')\n",
    "line = data.readline()\n",
    "while line:\n",
    "    if line.startswith('WIKI'):\n",
    "        _, pred_n = line.rstrip('\\n').split('\\t')\n",
    "        sent = data.readline().rstrip('\\n')\n",
    "        for i in range(int(pred_n)):\n",
    "            pred_idx, _, n_qs = data.readline().strip('\\n').split('\\t')\n",
    "            for j in range(int(n_qs)):\n",
    "                _= data.readline()\n",
    "                \n",
    "                \n",
    "            sent_pred_pairs_train[sent].append(pred_idx)\n",
    "    line = data.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEV SET\n",
    "\n",
    "sent_pred_pairs_dev = defaultdict(list)\n",
    "data = open('wiki1.dev.qa')\n",
    "line = data.readline()\n",
    "while line:\n",
    "    if line.startswith('WIKI'):\n",
    "        _, pred_n = line.rstrip('\\n').split('\\t')\n",
    "        sent = data.readline().rstrip('\\n')\n",
    "        for i in range(int(pred_n)):\n",
    "            pred_idx, _, n_qs = data.readline().strip('\\n').split('\\t')\n",
    "            for j in range(int(n_qs)):\n",
    "                _= data.readline()\n",
    "                \n",
    "                \n",
    "            sent_pred_pairs_dev[sent].append(pred_idx)\n",
    "    line = data.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pred_pairs_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждому предложению соответсвует один или больше индексов предикатов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1. \n",
    "Преобразуйте этот формат в пригодный для обучения классификаторов. Обучите какой-нибудь простой и оцените на отложенной выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем решить задачу целиком."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs_train = []\n",
    "data = open('wiki1.train.qa')\n",
    "line = data.readline()\n",
    "while line:\n",
    "    if line.startswith('WIKI'):\n",
    "        _, pred_n = line.rstrip('\\n').split('\\t')\n",
    "        sent = data.readline()\n",
    "        for i in range(int(pred_n)):\n",
    "            pred_idx, _, n_qs = data.readline().strip('\\n').split('\\t')\n",
    "            for j in range(int(n_qs)):\n",
    "                q, a = data.readline().split('?')\n",
    "                q = ' '.join([w for w in q.split('\\t') if w !='_'])\n",
    "                answers = a.lstrip('\\t').rstrip('\\n').split('###')\n",
    "                qa_pairs_train.append((sent, pred_idx, q, answers))\n",
    "    line = data.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs_dev = []\n",
    "data = open('wiki1.dev.qa')\n",
    "line = data.readline()\n",
    "while line:\n",
    "    if line.startswith('WIKI'):\n",
    "        _, pred_n = line.rstrip('\\n').split('\\t')\n",
    "        sent = data.readline()\n",
    "        for i in range(int(pred_n)):\n",
    "            pred_idx, _, n_qs = data.readline().strip('\\n').split('\\t')\n",
    "            for j in range(int(n_qs)):\n",
    "                q, a = data.readline().split('?')\n",
    "                \n",
    "                q = ' '.join([w for w in q.split('\\t') if w !='_'])\n",
    "                answers = a.lstrip('\\t').rstrip('\\n').split('###')\n",
    "                qa_pairs_dev.append((sent, pred_idx, q, answers))\n",
    "    line = data.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сопоставить ответы и предложения нам понадобится вот такая функция. Она находит, где в строке встречается ответ, а затем разбивает на токены, сохраняя отметки ответов. Если ответ состоит из нескольких токенов, то они маркируются BI тэгами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def label_text(text, answers, tag):\n",
    "    labels = []\n",
    "    text = re.sub('  +', ' ', text)\n",
    "\n",
    "    for word in answers:\n",
    "        word = word.strip()\n",
    "        start = text.find(word)\n",
    "        if start >= 0:\n",
    "            labels.append((start, start+len(word)))\n",
    "    \n",
    "    \n",
    "    words = text.split()\n",
    "    if not labels:\n",
    "        return [(word, 'O') for word in words]\n",
    "    \n",
    "    spans = []\n",
    "    i = 0\n",
    "    for word in words:\n",
    "        strip_word_right = word.rstrip(string.punctuation)\n",
    "        strip_word_left = word.lstrip(string.punctuation)\n",
    "\n",
    "        spans.append((i, i+len(word)-len(strip_word_left), i+len(word), i+len(strip_word_right)))\n",
    "        i += len(word)\n",
    "        i += 1\n",
    "\n",
    "    tags = []\n",
    "    for span in spans:\n",
    "        for label in labels:\n",
    "            if (span[0] >= label[0] or span[1] >= label[0]) \\\n",
    "              and (span[2] <= label[1] or span[3] <= label[1]):\n",
    "                tags.append(tag)\n",
    "                break\n",
    "        else:\n",
    "            tags.append('O')\n",
    "    bio_tags = []\n",
    "    inside = False\n",
    "    for tag in tags:\n",
    "        if tag != 'O':\n",
    "            if inside:\n",
    "                bio_tags.append(tag+'-I')\n",
    "            else:\n",
    "                bio_tags.append(tag+'-B')\n",
    "                inside = True\n",
    "        else:\n",
    "            bio_tags.append(tag)\n",
    "            inside = False\n",
    "    \n",
    "    return list(zip(words, bio_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs_train[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs_train[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проще всего понять как это работает на примере."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_text(qa_pairs_train[0][0], qa_pairs_train[0][-1], 'ANSWER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще эта задача требует сложных методов решения, таких как нейронки. Но мы попробуем решить её стандартными подходами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем все в обучающую выборку такого формата - каждый строка это текущее слово, которому нужно предсказать тэг. В качестве признаков будем использовать ещё предыдущее слово, предыдущий тэг и вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = []\n",
    "train_tags = []\n",
    "train_previous_words = []\n",
    "train_previous_tags = []\n",
    "train_questions = []\n",
    "\n",
    "for pair in qa_pairs_train:\n",
    "    sent = pair[0].replace(' , ',  ', ')\n",
    "    answers = pair[-1]\n",
    "    q = pair[2]\n",
    "    \n",
    "    tagged = [('<START>', '<START>'), ] + label_text(sent, answers, 'ANS')\n",
    "    if not any([tag.startswith('ANS') for word, tag in tagged]):\n",
    "        continue\n",
    "    \n",
    "    for i in range(1, len(tagged)):\n",
    "        train_words.append(tagged[i][0])\n",
    "        train_tags.append(tagged[i][1])\n",
    "        train_previous_words.append(tagged[i-1][0])\n",
    "        train_previous_tags.append(tagged[i-1][1])\n",
    "        train_questions.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_words = []\n",
    "dev_tags = []\n",
    "dev_previous_words = []\n",
    "dev_previous_tags = []\n",
    "dev_questions = []\n",
    "\n",
    "for pair in qa_pairs_dev:\n",
    "    sent = pair[0].replace(' , ',  ', ')\n",
    "    answers = pair[-1]\n",
    "    q = pair[2]\n",
    "    \n",
    "    tagged = [('<START>', '<START>'), ] + label_text(sent, answers, 'ANS')\n",
    "    if not any([tag.startswith('ANS') for word, tag in tagged]):\n",
    "        continue\n",
    "    \n",
    "    for i in range(1, len(tagged)):\n",
    "        dev_words.append(tagged[i][0])\n",
    "        dev_tags.append(tagged[i][1])\n",
    "        dev_previous_words.append(tagged[i-1][0])\n",
    "        dev_previous_tags.append(tagged[i-1][1])\n",
    "        dev_questions.append(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь всё это векторизуем. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тэги кодируем через onehot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREVIOUS TAG ENCODING\n",
    "\n",
    "\n",
    "lenc_prev_tag = LabelEncoder()\n",
    "int_prev_tag_enc = lenc_prev_tag.fit_transform(train_previous_tags)\n",
    "onehot_prev_tag = OneHotEncoder(sparse=True)\n",
    "int_prev_tag = int_prev_tag_enc.reshape(len(int_prev_tag_enc), 1)\n",
    "\n",
    "X_prev_tag_train = onehot_prev_tag.fit_transform(int_prev_tag)\n",
    "\n",
    "int_prev_tag_enc_dev = lenc_prev_tag.transform(dev_previous_tags)\n",
    "int_prev_tag_dev = int_prev_tag_enc_dev.reshape(\n",
    "                                     len(int_prev_tag_enc_dev),1)\n",
    "\n",
    "X_prev_tag_dev = onehot_prev_tag.transform(int_prev_tag_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слова через CountVectorizer на символах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORD ENCODING\n",
    "cv_word = CountVectorizer(ngram_range=(2,4), analyzer='char', max_features=5000)\n",
    "cv_word.fit(train_words)\n",
    "\n",
    "X_word_train = cv_word.transform(train_words)\n",
    "\n",
    "X_word_dev = cv_word.transform(dev_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREVIOUS WORD ENCODING\n",
    "cv_prev_word = CountVectorizer(ngram_range=(2,4), analyzer='char', max_features=5000)\n",
    "cv_prev_word.fit(train_previous_words)\n",
    "\n",
    "X_prev_word_train = cv_prev_word.transform(train_previous_words)\n",
    "\n",
    "X_prev_word_dev = cv_prev_word.transform(dev_previous_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вопросы через CountVectorizer на словах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION WORD ENCODING\n",
    "uniq_questions = set(train_questions)\n",
    "cv_question = CountVectorizer(max_features=5000)\n",
    "cv_question.fit(uniq_questions)\n",
    "\n",
    "X_questions_train = cv_question.transform(train_questions)\n",
    "X_questions_dev = cv_question.transform(dev_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скливаем все в одну большую матрицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = hstack([X_word_train,\n",
    "                  X_prev_tag_train,\n",
    "                  X_prev_word_train,\n",
    "                  X_questions_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_dev = hstack([X_word_dev,\n",
    "                  X_prev_tag_dev,\n",
    "                  X_prev_word_dev,\n",
    "                  X_questions_dev])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем что-то и смотрим на качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegressionCV(class_weight='balanced')\n",
    "clf.fit(X_train, train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(dev_tags, clf.predict(X_test_svd)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно проверить как это работает на каком-то конкретном примере."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(word, prev_tag, prev_word, question):\n",
    "    int_prev_tag_enc = lenc_prev_tag.transform(\n",
    "                                   [prev_tag])\n",
    "    int_prev_tag = int_prev_tag_enc.reshape(\n",
    "                                         len(int_prev_tag_enc),1)\n",
    "\n",
    "    X_prev_tag = onehot_prev_tag.transform(int_prev_tag)\n",
    "\n",
    "    X_word = cv_word.transform([word])\n",
    "    X_prev_word = cv_prev_word.transform([prev_word])\n",
    "    X_question = cv_question.transform([question])\n",
    "\n",
    "    X = hstack([X_word,\n",
    "                  X_prev_tag,\n",
    "                  X_prev_word,\n",
    "                  X_question])\n",
    "    \n",
    "    pred = clf.predict(X)[0]\n",
    "    \n",
    "    return pred\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs_dev[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задаем предложение и вопрос. Кодируем обученными векторайзерами и предиктим по одному тэгу, передавая предсказанный в следующих шаг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = qa_pairs_dev[10][0]\n",
    "q = 'how many Israeli were killed'\n",
    "\n",
    "sent_tokens = ['<START>'] + sent.split()\n",
    "tags_pred = ['<START>']\n",
    "\n",
    "for i in range(1, len(sent_tokens)):\n",
    "    word = sent_tokens[i]\n",
    "    prev_word = sent_tokens[i-1]\n",
    "    prev_tag = tags_pred[i-1]\n",
    "    \n",
    "    pred = vectorize(word, prev_tag, prev_word, q)\n",
    "    \n",
    "    tags_pred.append(pred)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(sent_tokens, tags_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.\n",
    "Попробуйте улучшить модель любыми способами. Если данных слишком много используйте только какую-то часть пар."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
