{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Извлечение ключевых слов\n",
    "\n",
    "Или keyword extraction - это упрощенный вариант задачи саммаризации (иногда это называется реферирование), т.е. извлечения главной информации из текста. Настоящая саммаризация подразумевает, что главная информация описывается нормальными полными предложениями, но сделать это очень сложно. Извлечь только основные слова проще и задачу это решает тоже достаточно хорошо (по нескольким словам сразу приблизительно понятно, о чем текст и читать ключевые слова сильно быстрее, чем даже самое хорошее саммари). Еще одно преимущество ключевых слов - это то, что их удобно использовать в стандартном поиске (который работает только со словами и не анализирует последовательности). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одному тексту разные люди могут приписать разные ключевые слова, поэтому оценивать извлечение ключевых слов сложнее, чем обычную классификацию. Нужно либо для каждого текста иметь несколько наборов ключевых слов, либо брать тексты с ключевыми словами из разных источников. В любом случае с метрикам нужно быть аккуратными и смотреть больше на соотношение, чем на абсолютные значения. Ну и проверять полученные алгоритмы в реальной задаче или хотя бы на глаз. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "morph = MorphAnalyzer()\n",
    "stops = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем данные вот отсюда - https://github.com/mannefedov/ru_kw_eval_datasets Там лежат 4 датасета (статьи с хабра, с Russia Today, Независимой газеты и научные статьи с Киберленинки). Датасет НГ самый маленький, поэтому возьмем его в качестве примера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-25 11:28:55--  https://github.com/mannefedov/ru_kw_eval_datasets/raw/master/data/ng_0.jsonlines.zip\n",
      "Распознаётся github.com (github.com)… 140.82.121.3\n",
      "Подключение к github.com (github.com)|140.82.121.3|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 302 Found\n",
      "Адрес: https://raw.githubusercontent.com/mannefedov/ru_kw_eval_datasets/master/data/ng_0.jsonlines.zip [переход]\n",
      "--2021-03-25 11:28:55--  https://raw.githubusercontent.com/mannefedov/ru_kw_eval_datasets/master/data/ng_0.jsonlines.zip\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 2987145 (2,8M) [application/zip]\n",
      "Сохранение в: «ng_0.jsonlines.zip»\n",
      "\n",
      "ng_0.jsonlines.zip  100%[===================>]   2,85M  4,40MB/s    за 0,6s    \n",
      "\n",
      "2021-03-25 11:28:56 (4,40 MB/s) - «ng_0.jsonlines.zip» сохранён [2987145/2987145]\n",
      "\n",
      "--2021-03-25 11:28:56--  https://github.com/mannefedov/ru_kw_eval_datasets/raw/master/data/ng_1.jsonlines.zip\n",
      "Распознаётся github.com (github.com)… 140.82.121.3\n",
      "Подключение к github.com (github.com)|140.82.121.3|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 302 Found\n",
      "Адрес: https://raw.githubusercontent.com/mannefedov/ru_kw_eval_datasets/master/data/ng_1.jsonlines.zip [переход]\n",
      "--2021-03-25 11:28:56--  https://raw.githubusercontent.com/mannefedov/ru_kw_eval_datasets/master/data/ng_1.jsonlines.zip\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 3074290 (2,9M) [application/zip]\n",
      "Сохранение в: «ng_1.jsonlines.zip»\n",
      "\n",
      "ng_1.jsonlines.zip  100%[===================>]   2,93M  3,76MB/s    за 0,8s    \n",
      "\n",
      "2021-03-25 11:28:57 (3,76 MB/s) - «ng_1.jsonlines.zip» сохранён [3074290/3074290]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/mannefedov/ru_kw_eval_datasets/raw/master/data/ng_0.jsonlines.zip\n",
    "!wget https://github.com/mannefedov/ru_kw_eval_datasets/raw/master/data/ng_1.jsonlines.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ng_0.jsonlines.zip\n",
      "  inflating: ng_0.jsonlines          \n",
      "Archive:  ng_1.jsonlines.zip\n",
      "  inflating: ng_1.jsonlines          \n"
     ]
    }
   ],
   "source": [
    "!unzip ng_0.jsonlines.zip \n",
    "!unzip ng_1.jsonlines.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH_TO_DATA = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join(PATH_TO_DATA, file) for file in os.listdir(PATH_TO_DATA) if file.endswith('jsonlines')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим файлы в один датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([pd.read_json(file, lines=True) for file in files][:5], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1987, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[школа, образовательные стандарты, литература, история, фгос]</td>\n",
       "      <td>Ольга Васильева обещала \"НГ\" не перегружать школьников</td>\n",
       "      <td>https://amp.ng.ru/?p=http://www.ng.ru/education/2018-03-22/8_7195_school.html</td>\n",
       "      <td>В среду состоялось отложенное заседание Совета по федеральным государственным образовательным стандартам (ФГОС) при Министерстве образования и науки РФ. Собрание должно было состояться еще в понедельник, но было перенесено по просьбе членов совета. И вот пришло сообщение, что общественники выразили согласие с позицией министерства. Новые ФГОСы приняты.\\nНа вчерашнем заседании был принят ФГОС по начальной общеобразовательной школе. До 28 марта продлятся косультации по ФГОСам для средней школы.\\nНапомним, что накануне Гильдия словесников разместила открытое письмо на имя министра образования и науки РФ Ольги Васильевой. По мнению авторов письма, новые ФГОСы грубо нарушают права детей, уже проучившихся по существующему стандарту до 6-го класса. Приняв новый стандарт, Министерство образования дает право контролирующим органам ловить детей на незнании большого списка произведений (235 за пять лет обучения). «Это исключает возможность полноценного их освоения, создает риск формального, п...</td>\n",
       "      <td>Глава Минобрнауки считает, что в нездоровом ажиотаже вокруг новых образовательных стандартов виноваты издательства учебной литературы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[красота, законы]</td>\n",
       "      <td>У красоты собственные закон и воля</td>\n",
       "      <td>https://amp.ng.ru/?p=http://www.ng.ru/style/2018-03-19/8_7192_beauty.html</td>\n",
       "      <td>Хорошо, когда красота в глазах смотрящего живет свободно или хотя бы занимает широкий угол зрения. Плохо было б, если б она вовсе не озаряла своим светом космическую темень пустоты зрачка. Слава богу, такое вряд ли возможно. \\nА случается, что красота уходит. Почему вдруг? И куда она девается, когда в один из философских обходов своего организма вы, еще недавно гордый ее обладатель, обескураженно ее  недосчитываетесь? \\nВообразите: прелестнейшее из созданий – ваша кошка пластичнейшими движениями рвет банкноту за банкнотой, забирается на карниз по шелковой занавеске или отгрызает полпаспорта. Где, скажите, теперь красота этой кошки? Или другой пример – с зазнобой сердца. Предмет романтичнейших грез наконец-то садится с вами на заветную скамейку в парке – закат, пение птах… И тут он силой своего обаяния с оглушительным плюхом обрушивает вокруг вас красоту и гармонию столетних дубов, тополей и прочего. Где, спрашивается, красота момента? \\nЕсли от сказки после того, как ее рассказали,...</td>\n",
       "      <td>О живительной пользе укорота при выборе между плохим и хорошим</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[юзефович, гражданская война, пепеляев, якутия]</td>\n",
       "      <td>Апокалиптический бунт</td>\n",
       "      <td>https://amp.ng.ru/?p=http://www.ng.ru/zavisimaya/2017-12-19/15_7139_bunt.html</td>\n",
       "      <td>Когда-то Леонид Юзефович написал книгу о монгольской эпопее барона Унгерна «Самодержец пустыни» – она стала интеллектуальным бестселлером и классикой жанра – документальный роман. В то время автор попутно изучал и историю вооруженного восстания в Якутии в 1922–1923 годах под руководством Анатолия Пепеляева. И вот теперь из «якутского» материала сложилась отдельная книга. Тема ее для нынешнего читателя поистине раритетна. Ведь воевавший где-то на самом краю страны Пепеляев практически забыт, притом что о борьбе с ним когда-то в СССР выходили статьи и книги. В памяти потомков, образно говоря, от Пепеляева остался только пепел.\\nЮзефович воскрешает в памяти не только его военные дела, но и человеческие черты. Этот провинциальный интеллигент, неврастеник и фаталист, начал восстание, практически не имея шансов на успех. Однако силою недюжинной харизмы Пепеляев сумел собрать вокруг себя многих боевых офицеров, таежных охотников и недовольных новыми порядками аборигенов. Для своих 32 лет ...</td>\n",
       "      <td>Крепость из тел и призрак независимой Якутии</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        keywords  \\\n",
       "0  [школа, образовательные стандарты, литература, история, фгос]   \n",
       "1                                              [красота, законы]   \n",
       "2                [юзефович, гражданская война, пепеляев, якутия]   \n",
       "\n",
       "                                                    title  \\\n",
       "0  Ольга Васильева обещала \"НГ\" не перегружать школьников   \n",
       "1                      У красоты собственные закон и воля   \n",
       "2                                   Апокалиптический бунт   \n",
       "\n",
       "                                                                             url  \\\n",
       "0  https://amp.ng.ru/?p=http://www.ng.ru/education/2018-03-22/8_7195_school.html   \n",
       "1      https://amp.ng.ru/?p=http://www.ng.ru/style/2018-03-19/8_7192_beauty.html   \n",
       "2  https://amp.ng.ru/?p=http://www.ng.ru/zavisimaya/2017-12-19/15_7139_bunt.html   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   content  \\\n",
       "0  В среду состоялось отложенное заседание Совета по федеральным государственным образовательным стандартам (ФГОС) при Министерстве образования и науки РФ. Собрание должно было состояться еще в понедельник, но было перенесено по просьбе членов совета. И вот пришло сообщение, что общественники выразили согласие с позицией министерства. Новые ФГОСы приняты.\\nНа вчерашнем заседании был принят ФГОС по начальной общеобразовательной школе. До 28 марта продлятся косультации по ФГОСам для средней школы.\\nНапомним, что накануне Гильдия словесников разместила открытое письмо на имя министра образования и науки РФ Ольги Васильевой. По мнению авторов письма, новые ФГОСы грубо нарушают права детей, уже проучившихся по существующему стандарту до 6-го класса. Приняв новый стандарт, Министерство образования дает право контролирующим органам ловить детей на незнании большого списка произведений (235 за пять лет обучения). «Это исключает возможность полноценного их освоения, создает риск формального, п...   \n",
       "1  Хорошо, когда красота в глазах смотрящего живет свободно или хотя бы занимает широкий угол зрения. Плохо было б, если б она вовсе не озаряла своим светом космическую темень пустоты зрачка. Слава богу, такое вряд ли возможно. \\nА случается, что красота уходит. Почему вдруг? И куда она девается, когда в один из философских обходов своего организма вы, еще недавно гордый ее обладатель, обескураженно ее  недосчитываетесь? \\nВообразите: прелестнейшее из созданий – ваша кошка пластичнейшими движениями рвет банкноту за банкнотой, забирается на карниз по шелковой занавеске или отгрызает полпаспорта. Где, скажите, теперь красота этой кошки? Или другой пример – с зазнобой сердца. Предмет романтичнейших грез наконец-то садится с вами на заветную скамейку в парке – закат, пение птах… И тут он силой своего обаяния с оглушительным плюхом обрушивает вокруг вас красоту и гармонию столетних дубов, тополей и прочего. Где, спрашивается, красота момента? \\nЕсли от сказки после того, как ее рассказали,...   \n",
       "2  Когда-то Леонид Юзефович написал книгу о монгольской эпопее барона Унгерна «Самодержец пустыни» – она стала интеллектуальным бестселлером и классикой жанра – документальный роман. В то время автор попутно изучал и историю вооруженного восстания в Якутии в 1922–1923 годах под руководством Анатолия Пепеляева. И вот теперь из «якутского» материала сложилась отдельная книга. Тема ее для нынешнего читателя поистине раритетна. Ведь воевавший где-то на самом краю страны Пепеляев практически забыт, притом что о борьбе с ним когда-то в СССР выходили статьи и книги. В памяти потомков, образно говоря, от Пепеляева остался только пепел.\\nЮзефович воскрешает в памяти не только его военные дела, но и человеческие черты. Этот провинциальный интеллигент, неврастеник и фаталист, начал восстание, практически не имея шансов на успех. Однако силою недюжинной харизмы Пепеляев сумел собрать вокруг себя многих боевых офицеров, таежных охотников и недовольных новыми порядками аборигенов. Для своих 32 лет ...   \n",
       "\n",
       "                                                                                                                                 summary  \n",
       "0  Глава Минобрнауки считает, что в нездоровом ажиотаже вокруг новых образовательных стандартов виноваты издательства учебной литературы  \n",
       "1                                                                         О живительной пользе укорота при выборе между плохим и хорошим  \n",
       "2                                                                                           Крепость из тел и призрак независимой Якутии  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1987, 8)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждой статье приписано какое-то количество ключевых слов. **Наша задача - придумать как извлекать точно такой же список автоматически.**\n",
    "Зададим несколько метрик, по которым будем определять качество извлекаемых ключевых слов - точность, полноту, ф1-меру и меру жаккарда."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true_kws, predicted_kws):\n",
    "    assert len(true_kws) == len(predicted_kws)\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    jaccards = []\n",
    "    \n",
    "    for i in range(len(true_kws)):\n",
    "        \n",
    "        true_kw = set(true_kws[i])\n",
    "        predicted_kw = set(predicted_kws[i])\n",
    "        \n",
    "        tp = len(true_kw & predicted_kw)\n",
    "        union = len(true_kw | predicted_kw)\n",
    "        fp = len(predicted_kw - true_kw)\n",
    "        fn = len(true_kw - predicted_kw)\n",
    "        \n",
    "        if (tp+fp) == 0:\n",
    "            prec = 0\n",
    "        else:\n",
    "            prec = tp / (tp + fp)\n",
    "        \n",
    "        if (tp+fn) == 0:\n",
    "            rec = 0\n",
    "        else:\n",
    "            rec = tp / (tp + fn)\n",
    "        if (prec+rec) == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = (2*(prec*rec))/(prec+rec)\n",
    "            \n",
    "        jac = tp / union\n",
    "        \n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1s.append(f1)\n",
    "        jaccards.append(jac)\n",
    "    print('Precision - ', round(np.mean(precisions), 2))\n",
    "    print('Recall - ', round(np.mean(recalls), 2))\n",
    "    print('F1 - ', round(np.mean(f1s), 2))\n",
    "    print('Jaccard - ', round(np.mean(jaccards), 2))\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что всё работает как надо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  1.0\n",
      "Recall -  1.0\n",
      "F1 -  1.0\n",
      "Jaccard -  1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['keywords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тупое решение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте не будем долго думать и тестрировать первое, что приходит в голову."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем первые 5 слов из заголовка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.06\n",
      "Recall -  0.05\n",
      "F1 -  0.05\n",
      "Jaccard -  0.03\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['title'].apply(lambda x: x.lower().split()[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.06\n",
      "Recall -  0.06\n",
      "F1 -  0.05\n",
      "Jaccard -  0.03\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['title'].apply(lambda x: x.lower().split()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем взять самые частотные слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.02\n",
      "Recall -  0.04\n",
      "F1 -  0.02\n",
      "Jaccard -  0.01\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['content'].apply(lambda x: \n",
    "                                                 [x[0] for x in Counter(x.lower().split()).most_common(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или вообще рандомные слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.06\n",
      "Recall -  0.05\n",
      "F1 -  0.05\n",
      "Jaccard -  0.03\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['title'].apply(lambda x: \n",
    "                                                 np.random.choice(list(set(x.lower().split())), 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте посмотрим, что вообще извлекается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      [ольга, васильева, обещала, \"нг\", не, перегружать, школьников]\n",
       "1                                           [у, красоты, собственные, закон, и, воля]\n",
       "2                                                            [апокалиптический, бунт]\n",
       "3                   [f1., предсказать, результаты, гран-при, испании, было, несложно]\n",
       "4                                               [возвращение, в, небесное, отечество]\n",
       "5                                       [практическая, медицина, с, большим, будущим]\n",
       "6                     [бумажный, носитель., вселенная,, человек,, язык, человеческий]\n",
       "7                                      [в, багдаде, неспокойно,, а, будет, еще, хуже]\n",
       "8    [ученые, предсказывают, появление, интуиции, у, искусственных, нейронных, сетей]\n",
       "9                                  [россияне, от, вступления, в, вто, не, пострадают]\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title'].apply(lambda x: x.lower().split()[:10]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            [и, в, –, по, на, что, с, о, к, не]\n",
       "1                 [и, в, не, что, –, на, красота, вы, если, как]\n",
       "2                     [в, и, не, на, –, от, о, из, пепеляев, но]\n",
       "3                        [в, не, и, на, что, но, это, как, с, к]\n",
       "4                          [и, в, –, не, он, с, на, что, а, его]\n",
       "5                       [и, в, на, –, с, что, не, это, как, уже]\n",
       "6                           [–, и, в, не, на, из, по, что, с, а]\n",
       "7                       [в, и, не, на, но, ли, ираке, что, -, а]\n",
       "8                           [и, в, на, о, с, –, как, а, что, ai]\n",
       "9    [в, на, по, и, что, кристалина, –, россии, георгиева, том,]\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['content'].apply(lambda x: [x[0] for x in Counter(x.lower().split()).most_common(10)]).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда извлекаются частотные слова, то список почти полностью состоит из всяких стоп-слов. Также из-за плохой токенизации некоторые слова в обоих списках - пунктуация или слова с пунктуацией на концах. К тому же извлекаемые слова ненормализованы, а правильные ключевые слова - наоборот."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация, удаление стоп-слов и нормализация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_norm'] = data['content'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title_norm'] = data['title'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           [ольга, васильев, обещать, нг, перегружать, школьник]\n",
       "1                                             [красота, собственный, закон, воля]\n",
       "2                                                        [апокалиптический, бунт]\n",
       "3                       [f1, предсказать, результат, гран-при, испания, несложно]\n",
       "4                                              [возвращение, небесный, отечество]\n",
       "5                                      [практический, медицина, больший, будущий]\n",
       "6                    [бумажный, носитель, вселенная, человек, язык, человеческий]\n",
       "7                                                     [багдад, неспокойно, худой]\n",
       "8    [учёный, предсказывать, появление, интуиция, искусственный, нейронный, сеть]\n",
       "9                                        [россиянин, вступление, вто, пострадать]\n",
       "Name: title_norm, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title_norm'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем те же самые методы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.11\n",
      "Recall -  0.22\n",
      "F1 -  0.14\n",
      "Jaccard -  0.08\n"
     ]
    }
   ],
   "source": [
    "# топ 10 частотных слов статьи\n",
    "evaluate(data['keywords'], data['content_norm'].apply(lambda x: [x[0] for x in Counter(x).most_common(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.13\n",
      "Recall -  0.13\n",
      "F1 -  0.12\n",
      "Jaccard -  0.07\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'],data['title_norm'].apply(lambda x: x[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           [ольга, васильев, обещать, нг, перегружать, школьник]\n",
       "1                             [красота, собственный, закон, воля]\n",
       "2                                        [апокалиптический, бунт]\n",
       "3       [f1, предсказать, результат, гран-при, испания, несложно]\n",
       "4                              [возвращение, небесный, отечество]\n",
       "                                  ...                            \n",
       "1982          [орбан, призвать, остановить, исламский, экспансия]\n",
       "1983                  [парижский, соглашение, трудность, перевод]\n",
       "1984                                     [троцкий, убить, кирпич]\n",
       "1985                                [кино, город, сверкать, снег]\n",
       "1986                       [информационный, война, цифровой, мир]\n",
       "Name: title_norm, Length: 1987, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title_norm'].apply(lambda x: x[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество сильно улучшилось! Можно теперь ещё раз посмотреть, что плохого извлекается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [стандарт, который, источник, образовательный, фгоса, ольга, васильев, результат, предмет, исторический]\n",
       "1                                          [красота, ваш, глаз, отчаяние, уйти, это, порыв, кошка, свой, один]\n",
       "2                               [пепеляев, юзеф, книга, якутия, восстание, год, самый, война, место, когда-то]\n",
       "3                              [гонка, команда, это, сказать, пилот, ferrari, mclaren, сезон, mercedes, время]\n",
       "4                                          [есенин, поэт, клюев, год, свой, это, жизнь, который, смерть, 1925]\n",
       "5                        [наш, медицина, медицинский, это, кафедра, выпускник, работать, уровень, работа, год]\n",
       "6                               [книга, русский, человек, мозг, два, островной, говор, это, анатомия, который]\n",
       "7                     [ирак, война, американец, это, партизанский, войско, структура, единый, сказать, важный]\n",
       "8                       [который, нейросеть, свой, ai, клетка, искусственный, интеллект, самый, время, основа]\n",
       "9                  [россия, вступление, кристалина, георгиев, вто, банка, страна, переговоры, директор, слово]\n",
       "10                             [фильм, приз, который, картина, свой, кино, например, программа, один, медведь]\n",
       "11                                [это, каша, сидеть, столовый, который, человек, такой, начинать, рок, талон]\n",
       "12                            [место, псков, город, день, такой, князь, андрей, туризм, концепция, ганзейский]\n",
       "13                  [индонезия, который, страна, год, мусульманский, экстремизм, терроризм, это, человек, сша]\n",
       "14                                      [это, свой, такой, который, игра, каждый, другой, друг, самый, парень]\n",
       "15                      [выбор, путин, президент, признавать, сша, рф, россия, страна, лидер, демократический]\n",
       "16                         [водный, год, вода, качество, загрязнение, объект, страна, хозяйство, это, очистка]\n",
       "17           [школа, образование, год, москва, школьник, олимпиада, международный, московский, высокий, город]\n",
       "18                                   [который, фильм, один, стать, содерберг, слон, герой, это, сидеть, новый]\n",
       "19                                 [канада, один, швед, лундквист, ворота, счёт, свой, олимпийский, матч, два]\n",
       "Name: content_norm, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['content_norm'].apply(lambda x: [x[0] for x in Counter(x).most_common(10)]).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё остались некоторые стоп-слова. Вместо того, чтобы расширять список, давайте попробуем выкинуть несуществительные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
    "    words = [word.normal_form for word in words if word.tag.POS == 'NOUN']\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_norm'] = data['content'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.13\n",
      "Recall -  0.25\n",
      "F1 -  0.16\n",
      "Jaccard -  0.1\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['content_norm'].apply(lambda x: [x[0] for x in Counter(x).most_common(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               [стандарт, источник, фгоса, ольга, васильев, результат, предмет, школа, письмо, произведение]\n",
       "1                                 [красота, глаз, отчаяние, порыв, кошка, предмет, дело, руина, место, мечта]\n",
       "2                               [пепеляев, юзеф, книга, якутия, восстание, год, война, место, леонид, сибирь]\n",
       "3                               [гонка, команда, пилот, сезон, время, машина, место, круг, гран-при, испания]\n",
       "4                               [есенин, поэт, клюев, год, жизнь, смерть, сергей, человек, борода, мариенгоф]\n",
       "                                                        ...                                                  \n",
       "1982                       [ес, страна, политика, европа, брюссель, венгрия, правый, орбан, выборы, евросоюз]\n",
       "1983                         [выброс, развитие, цель, газ, уровень, год, соглашение, сторона, климат, страна]\n",
       "1984                          [коваль, юрий, писатель, книга, человек, слово, клуб, иосич, осина, милиционер]\n",
       "1985                             [фильм, режиссёр, фестиваль, приз, год, кино, дух, огонь, внимание, зритель]\n",
       "1986    [война, безопасность, средство, экономика, вопрос, проблема, страна, система, информация, технология]\n",
       "Name: content_norm, Length: 1987, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['content_norm'].apply(lambda x: [x[0] for x in Counter(x).most_common(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещу улучшения!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [стандарт, источник, фгоса, ольга, васильев, результат, предмет, школа, письмо, произведение]\n",
       "1                      [красота, глаз, отчаяние, порыв, кошка, предмет, дело, руина, место, мечта]\n",
       "2                    [пепеляев, юзеф, книга, якутия, восстание, год, война, место, леонид, сибирь]\n",
       "3                    [гонка, команда, пилот, сезон, время, машина, место, круг, гран-при, испания]\n",
       "4                    [есенин, поэт, клюев, год, жизнь, смерть, сергей, человек, борода, мариенгоф]\n",
       "5       [медицина, кафедра, выпускник, уровень, работа, год, практика, ординатор, рудна, обучение]\n",
       "6               [книга, человек, мозг, говор, анатомия, глава, звезда, вопрос, слово, азербайджан]\n",
       "7       [ирак, война, американец, войско, структура, принцип, взрыв, центр, террорист, количество]\n",
       "8    [нейросеть, клетка, интеллект, время, основа, изображение, задача, работа, система, внимание]\n",
       "9      [россия, вступление, кристалина, георгиев, вто, банка, страна, переговоры, директор, слово]\n",
       "Name: content_norm, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['content_norm'].apply(lambda x: [x[0] for x in Counter(x).most_common(10)]).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не очень значимые слова все ещё остались. Давайте попробуем отсеять стоп-слова с помощью tfidf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_norm_str'] = data['content_norm'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# можно заодно сделать нграммы\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit(data['content_norm_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем наши тексты в векторы, где на позиции i стоит tfidf коэффициент слова i из словаря."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_vectors = tfidf.transform(data['content_norm_str'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отсортируем векторы текстов по этим коэффициентам и возьмем топ-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## так как матрица в tfidf в спарс формате,  ее нельзя просто так отсортировать\n",
    "## перевести ее в обычный формат для всех данных тоже не получится - не хватит памяти\n",
    "## поэтому пройдем по строчкам, переведем строчку в обычный array и отсортируем ее\n",
    "keywords = []\n",
    "\n",
    "for row in range(texts_vectors.shape[0]):\n",
    "    row_data = texts_vectors.getrow(row)\n",
    "    top_inds = row_data.toarray().argsort()[0,:-11:-1]\n",
    "    keywords.append([id2word[w] for w in top_inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['фгоса',\n",
       "  'ольга васильев',\n",
       "  'стандарт',\n",
       "  'васильев',\n",
       "  'ольга',\n",
       "  'источник',\n",
       "  'предмет',\n",
       "  'событие явление',\n",
       "  'произведение',\n",
       "  'письмо'],\n",
       " ['красота',\n",
       "  'отчаяние',\n",
       "  'порыв',\n",
       "  'кошка',\n",
       "  'глаз',\n",
       "  'руина',\n",
       "  'порыв отчаяние',\n",
       "  'красота глаз',\n",
       "  'ваби',\n",
       "  'вопрос красота'],\n",
       " ['пепеляев',\n",
       "  'юзеф',\n",
       "  'якутия',\n",
       "  'восстание',\n",
       "  'книга',\n",
       "  'восстание якутия',\n",
       "  'пепеляев боец',\n",
       "  'анатолий пепеляев',\n",
       "  'начало 20',\n",
       "  'леонид юзеф']]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.13\n",
      "Recall -  0.24\n",
      "F1 -  0.16\n",
      "Jaccard -  0.09\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат ещё немного улучшился. Немного подросла точность. Теперь вместо стоп-слов в ключевые попадают имена и все такое. Иногда это хорощо, а иногда нет (собянин - может быть ключевым словом, а дарья - вряд ли)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем этот результат за **baseline.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision -  0.13\n",
    "Recall -  0.25\n",
    "F1 -  0.16\n",
    "Jaccard -  0.09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попробуем графы!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большая часть методов для извлечения ключевых слов основана на применении графов. Основная идея - каким-то образом перевести текст в граф, а затем каким-то образом расчитать важность каждого узла и вывести топ-N самых важных узлов.  \n",
    "\n",
    "Перевод текста в граф -  не тривиальная задача. Часто применяют такой подход - построим матрицу совстречаемости слов (в каком-то окне), эта матрица будет нашей матрицей смежности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выбора важных узлов часто используют простой randow walk. Алгоритм примерно такой:  \n",
    "1) Каким-то образом выбирается первый узел графа (например, случайно из равномерного распределения)  \n",
    "2) на основе связей этого узла с другими, выбирается следующий узел  \n",
    "3) шаг два повторяется некоторое количество раз (например, тысячу) __*чтобы не зацикливаться, с какой-то вероятностью мы случайно перескакиваем на другой узел (даже если он никак не связан с текущим, как в шаге 1)__  \n",
    "5) на каждом шаге мы сохраняем узел в котором находимся  \n",
    "6) в конце мы считаем в каких узлах мы были чаще всего и выводим top-N  \n",
    "\n",
    "\n",
    "Предполагается, что мы часто будем приходить в важные узлы графа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для наглядности реализуем этот подход без networkx. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kws(text, top=5, window_size=5, random_p=0.1):\n",
    "\n",
    "    vocab = set(text)\n",
    "    word2id = {w:i for i, w in enumerate(vocab)}\n",
    "    id2word = {i:w for i, w in enumerate(vocab)}\n",
    "    # преобразуем слова в индексы для удобства\n",
    "    ids = [word2id[word] for word in text]\n",
    "\n",
    "    # создадим матрицу совстречаемости\n",
    "    m = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    # пройдемся окном по всему тексту\n",
    "    for i in range(0, len(ids), window_size):\n",
    "        window = ids[i:i+window_size]\n",
    "        # добавим единичку всем парам слов в этом окне\n",
    "        for j, k in combinations(window, 2):\n",
    "            # чтобы граф был ненаправленный \n",
    "            m[j][k] += 1\n",
    "            m[k][j] += 1\n",
    "    \n",
    "    # нормализуем строки, чтобы получилась вероятность перехода\n",
    "    for i in range(m.shape[0]):\n",
    "        s = np.sum(m[i])\n",
    "        if not s:\n",
    "            continue\n",
    "        m[i] /= s\n",
    "    \n",
    "    # случайно выберем первое слова, а затем будет выбирать на основе полученых распределений\n",
    "    # сделаем так 5 раз и добавим каждое слово в счетчик\n",
    "    # чтобы не забиться в одном круге, иногда будет перескакивать на случайное слово\n",
    "    \n",
    "    c = Counter()\n",
    "    # начнем с абсолютного случайно выбранного элемента\n",
    "    n = np.random.choice(len(vocab))\n",
    "    for i in range(500): # если долго считается, можно уменьшить число проходов\n",
    "        \n",
    "        # c вероятностью random_p \n",
    "        # перескакиваем на другой узел\n",
    "        go_random = np.random.choice([0, 1], p=[1-random_p, random_p])\n",
    "        \n",
    "        if go_random:\n",
    "            n = np.random.choice(len(vocab))\n",
    "        \n",
    "        \n",
    "        ### \n",
    "        n = take_step(n, m)\n",
    "        # записываем узлы, в которых были\n",
    "        c.update([n])\n",
    "    \n",
    "    # вернем топ-N наиболее часто встретившихся сл\n",
    "    return [id2word[i] for i, count in c.most_common(top)]\n",
    "\n",
    "def take_step(n, matrix):\n",
    "    rang = len(matrix[n])\n",
    "    # выбираем узел из заданного интервала, на основе распределения из матрицы совстречаемости\n",
    "    if np.any(matrix[n]):\n",
    "        next_n = np.random.choice(range(rang), p=matrix[n])\n",
    "    else:\n",
    "        next_n = np.random.choice(range(rang))\n",
    "    return next_n\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1987/1987 [01:40<00:00, 19.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 38s, sys: 8.43 s, total: 1min 47s\n",
      "Wall time: 1min 40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "keywords_rw = data['content_norm'].progress_apply(lambda x: get_kws(x, 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.11\n",
      "Recall -  0.21\n",
      "F1 -  0.14\n",
      "Jaccard -  0.08\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords_rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [событие, письмо, стандарт, фгоса, школа, источник, васильев, процесс, произведение, ольга]\n",
       "1                [красота, глаз, кошка, отчаяние, место, порыв, занавеска, вопрос, дерево, предмет]\n",
       "2                [пепеляев, якутия, книга, юзеф, собрание, поэт, красных, начало, 20-ха, восстание]\n",
       "3                   [команда, гонка, пилот, круг, машина, борьба, сезон, время, испания, себастьян]\n",
       "4                          [есенин, год, поэт, жизнь, клюев, борода, гроб, будущее, тишь, исповедь]\n",
       "5         [кафедра, медицина, уровень, выпускник, гражданин, год, вопрос, качество, обучение, врач]\n",
       "6                     [глава, говор, вопрос, небо, человек, звезда, верея, исследование, мозг, мир]\n",
       "7          [ирак, война, штаб, узел, американец, снабжение, структура, хусейн, специалист, бригада]\n",
       "8    [нейросеть, клетка, время, задача, основа, изображение, интеллект, вычисление, фото, внимание]\n",
       "9       [переговоры, директор, вто, вступление, георгиев, россия, слово, кристалина, страна, линия]\n",
       "Name: content_norm, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_rw.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попбруем теперь важность считать с помощью какой-нибудь метрики из networkx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(text, window_size=5):\n",
    "    vocab = set(text)\n",
    "    word2id = {w:i for i, w in enumerate(vocab)}\n",
    "    id2word = {i:w for i, w in enumerate(vocab)}\n",
    "    # преобразуем слова в индексы для удобства\n",
    "    ids = [word2id[word] for word in text]\n",
    "\n",
    "    # создадим матрицу совстречаемости\n",
    "    m = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    # пройдемся окном по всему тексту\n",
    "    for i in range(0, len(ids), window_size):\n",
    "        window = ids[i:i+window_size]\n",
    "        # добавим единичку всем парам слов в этом окне\n",
    "        for j, k in combinations(window, 2):\n",
    "            # чтобы граф был ненаправленный \n",
    "            m[j][k] += 1\n",
    "            m[k][j] += 1\n",
    "    \n",
    "    return m, id2word\n",
    "\n",
    "def some_centrality_measure(text, window_size=5, topn=5):\n",
    "    \n",
    "    matrix, id2word = build_matrix(text, window_size)\n",
    "    G = nx.from_numpy_array(matrix)\n",
    "    # тут можно поставить любую метрику\n",
    "    # менять тут \n",
    "    node2measure = dict(nx.pagerank(G)) \n",
    "    \n",
    "    return [id2word[index] for index,measure in sorted(node2measure.items(), key=lambda x: -x[1])[:topn]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут использован PageRank как метрика центральности. Про другие можно узнать вот тут - https://networkx.github.io/documentation/stable/reference/algorithms/centrality.html\n",
    "\n",
    "Попробуйте разные метрики. Некоторые могут работать достаточно долго"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 29s, sys: 1.37 s, total: 3min 30s\n",
      "Wall time: 3min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "keyword_nx = data['content_norm'].apply(lambda x: some_centrality_measure(x, 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.12\n",
      "Recall -  0.24\n",
      "F1 -  0.16\n",
      "Jaccard -  0.09\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keyword_nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты не превосходят tfidf, но и не сильно уступают. Явно можно что-то доработать и превзойти baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готовое решение есть в gensim. Давайте попробуем его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_kws = data['content_norm'].apply(lambda x: keywords(' '.join(x)).split('\\n')[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.07\n",
      "Recall -  0.11\n",
      "F1 -  0.08\n",
      "Jaccard -  0.04\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], gensim_kws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша имплементация отработала получше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В семинаре использовался только небольшой кусочек данных. На всех данных пересчитайте baseline (tfidf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ваша задача - предложить 3 способа побить бейзлайн на всех данных.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нет никаких ограничений кроме:\n",
    "\n",
    "1) нельзя изменять метрику  \n",
    "2) решение должно быть воспроизводимым  \n",
    "3) способы дожны отличаться друг от друга не только гиперпараметрами (например, нельзя три раза поменять гиперпарамтры в TfidfVectorizer и сдать работу)  \n",
    "4) изменение количества извлекаемых слов не является улучшением (выберите одно значение и используйте только его)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве ответа нужно предоставить jupyter тетрадку с экспериментами (обязательное условие!) и описать каждую из идей в форме - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый реализованный и описанный способ оценивается в 3 балла. Дополнительный балл можно получить, если способы затрагивают разные аспекты решения (например, первая идея - улучшить нормализацию, вторая - улучшить способ представления текста в виде графа, третья - предложить способ удаления из топа идентичных ключевых слов (рф, россия))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно использовать мой код как основу, а можно придумать что-то полностью другое."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если у вас никак не получается побить бейзлайн вы можете предоставить реализацию и описание неудавшихся экспериментов (каждый оценивается в 1 балл)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В поисках идей можно почитать обзоры по теме (посмотрите еще статьи, в которых цитируются эти обзоры): https://www.semanticscholar.org/search?year%5B0%5D=2012&year%5B1%5D=2020&publicationType%5B0%5D=Reviews&q=keyword%20extraction&sort=relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Использовать доступные готовые решения тоже можно**. Так что погуглите перед тем, как приступать к заданию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
