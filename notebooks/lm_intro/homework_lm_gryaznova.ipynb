{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (сопоставимые или большие по объему). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг <start>  \n",
    "    - еще одна матрица не нужна, можно по строкам хронить биграмы, а по колонкам униграммы  \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d078056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6afcef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dvach = open('2ch_corpus.txt', encoding=\"utf8\").read()\n",
    "news = open('lenta.txt', encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a51f1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n",
    "\n",
    "#norm_dvach = normalize(dvach)\n",
    "norm_news = normalize(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9978ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#vocab_dvach = Counter(norm_dvach)\n",
    "vocab_news = Counter(norm_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4863576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#probas_dvach = Counter({word:c/len(norm_dvach) for word, c in vocab_dvach.items()})\n",
    "probas_news = Counter({word:c/len(norm_news) for word, c in vocab_news.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1b44cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "961ac261",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_news = [['<start>']+['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news)]\n",
    "sentences_news_test = sentences_news[:50]\n",
    "sentences_news = sentences_news[50:]\n",
    "#sentences_news = [['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05bbf207",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "trigrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))\n",
    "    trigrams_news.update(ngrammer(sentence, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5f14a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, csc_matrix, lil_matrix\n",
    "\n",
    "matrix_news = lil_matrix((len(bigrams_news), len(unigrams_news)))\n",
    "\n",
    "id2word_news = list(unigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "\n",
    "id2bigram_news = list(bigrams_news)\n",
    "bigram2id_news = {word:i for i, word in enumerate(id2bigram_news)}\n",
    "\n",
    "for ngram in trigrams_news:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = word1 + ' '+ word2 \n",
    "\n",
    "    matrix_news[bigram2id_news[bigram], word2id_news[word3]] =  (trigrams_news[ngram]/bigrams_news[bigram])\n",
    "\n",
    "matrix_news = csr_matrix(matrix_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2db52c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2bigram, id2word, bigram2id, n=100, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = bigram2id[start]\n",
    "    for i in range(n):\n",
    "        p = matrix[current_idx].toarray()[0]\n",
    "        chosen = np.random.choice(list(range(matrix.shape[1])), p = p)\n",
    "        text.append(id2word[chosen])\n",
    "        if id2word[chosen] == '<end>':\n",
    "            current_idx = bigram2id[start]\n",
    "        else:\n",
    "            part = id2bigram[current_idx] + ' ' + id2word[chosen]\n",
    "            part = ' '.join(part.split()[1:])\n",
    "            current_idx = bigram2id[part]\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d9e6a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 миллионов фунтов так что на нескольких серверах русской паутины на входных страницах поисковых машин оставалась одной из секций тепловоза товарного поезда взорвалась подложенная мина об этом риа новости \n",
      " как отметил суд дальнейшее содержание шутова под стражей было бы обратиться в международный суд стокгольма принял решение пригласить папу римского иоанна павла ii \n",
      " предполагается что в понедельник 25 октября поздно вечером завершился закрытый пленум московского городского суда в качестве чернорабочего \n",
      " этот документ не более 30 военнослужащих федеральных сил предотвратили диверсию на железнодорожных вокзалах в армавире и пятигорске в апреле этого года саммита группы восьми совета европы указал на\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_news, id2bigram_news, id2word_news, bigram2id_news, start='<start> <start>').replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "402d898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(probas):\n",
    "    p = np.exp(np.sum(probas))\n",
    "    N = len(probas)\n",
    "    \n",
    "    return p**(-1/N) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b8f19ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_perplexity(sentences):\n",
    "    all_perplexity = []\n",
    "    for sent in sentences_news_test:\n",
    "        probs = []\n",
    "        for ngram in ngrammer(sent, 3):\n",
    "            word1, word2, word3 = ngram.split()\n",
    "            bigram = word1 + ' '+word2\n",
    "            if ngram in trigrams_news and bigram in bigrams_news:\n",
    "                probs.append(np.log(trigrams_news[ngram]/bigrams_news[bigram]))\n",
    "            else:\n",
    "                probs.append(np.log(0.00001))\n",
    "                if perplexity(probs)!= np.inf: \n",
    "                    all_perplexity.append(perplexity(probs))\n",
    "                \n",
    "    return np.mean(all_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c343267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-29b2627a57da>:5: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return p**(-1/N)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64102.96600416621"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_perplexity(sentences_news_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36c44b",
   "metadata": {},
   "source": [
    "Прочитайте главу про языковое моделирование в книге Журафски и Мартина - https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b1bd8",
   "metadata": {},
   "source": [
    "Развернуто (в пределах 1000 знаков) ответьте на вопросы (по-русски):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cf844",
   "metadata": {},
   "source": [
    "1. Что можно делать с проблемой несловарных слов? В семинаре мы просто использовали какое-то маленькое значение вероятности, а какие есть другие способы?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d1c152",
   "metadata": {},
   "source": [
    "2. Что такое сглаживание (smoothing)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340714f8",
   "metadata": {},
   "source": [
    "1. Часто для языковых моделей используются системы \"закрытых\" словарей, то есть словарей с ограниченным количеством слов. Им противопоставляются системы \"открытых\" словарей, где мы можем моделировать потенциально незнакомые слова в тестовом наборе данных через добавление псевдо-слов. Есть два популярных способа обучить вероятности появления неизвестного слова.\n",
    "а) Первый способ - все же выбрать закрытый словарь заранее:\n",
    "- выбрать фиксированный словарь\n",
    "- на шаге нормализации в тестовом датасете конвертировать любое слово НЕ в этом датасете в <UNK> - \"unknown word token\".\n",
    "- посчитать вероятость <\"UNK\">\n",
    "б) Второй способ - если у нас нет готового словаря заранее, состоит в том, чтобы создать словарь путем замены слов в обучающем датасете на <\"UNK\"> в зависимости от их частотности. То есть если какие-то слова встречаются супер редко, их можно заменить на UNK. \n",
    "\n",
    "    2. Бывают ситуации, когда слова в тестовом датасете встречаются в контексте, в котором не встречались в обучающем. В таком случае, модель будет присваивать им нулевую вероятность встретиться в этом контексте. Чтобы этого избежать, берется вероятность у других событий и распределяется между событиями, которые не встречались. Это и называется сглаживанием.\n",
    "    Способов сглаживания много, но самый простой заключается в том, чтобы самостоятельно увеличить количество н-грамм на 1 перед тем, как считать вероятность. Так что теперь если н-грамма встречалась один раз - будем считать, что 2, если не встречалась вообще, будем считать, что встретилась один раз. Это называется сглаживанием Лапласа.\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
